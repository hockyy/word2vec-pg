{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modified code from: Alfan Farizki Wicaksono, S.T., M.Sc., Ph.D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as fun\n",
    "import numpy as np\n",
    "\n",
    "from google_play_scraper import Sort, reviews\n",
    "import csv\n",
    "\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "def clean_word(sentence):\n",
    "    sentence = wordpunct_tokenize(sentence)\n",
    "    cleaned = []\n",
    "    for word in sentence:\n",
    "        if(word.isalpha()):\n",
    "            cleaned.append(word.lower())\n",
    "    return cleaned\n",
    "\n",
    "def get_corpus_from_csv():\n",
    "    corpus = []\n",
    "    with open(\"dfyutup-cleanedfixversion3.csv\") as file_buffer:\n",
    "        csv_reader = list(csv.reader(file_buffer))\n",
    "        n = len(csv_reader)\n",
    "        for i in range(1, n):\n",
    "            corpus.append(clean_word(csv_reader[i][2]))\n",
    "    return corpus\n",
    "\n",
    "    \n",
    "def get_corpus_google_play(cnt = 100):\n",
    "    result, _ = reviews(\n",
    "        'com.apps.MyXL',\n",
    "        lang='id', \n",
    "        country='id', \n",
    "        sort=Sort.MOST_RELEVANT, \n",
    "        count=cnt, \n",
    "        filter_score_with=None\n",
    "    )\n",
    "    return [clean_word(konten[\"content\"]) for konten in result]\n",
    "\n",
    "# print(get_corpus_google_play())\n",
    "# print(get_corpus_from_csv())\n",
    "\n",
    "corpus = get_corpus_from_csv()\n",
    "# print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rick\n",
      "lu\n",
      "anak\n",
      "ya\n",
      "itu\n",
      "bang\n",
      "gw\n",
      "sub\n",
      "kontol\n",
      "apa\n",
      "di\n",
      "mu\n",
      "muka\n",
      "lo\n",
      "anjing\n",
      "gue\n",
      "nonton\n",
      "youtube\n",
      "suka\n",
      "gaya\n",
      "haha\n",
      "wkwkwkw\n",
      "buat\n",
      "haters\n",
      "semua\n",
      "sok\n",
      "video\n",
      "erickolim\n",
      "kalo\n",
      "gk\n",
      "ngapain\n",
      "ditonton\n",
      "eriko\n",
      "juga\n",
      "dia\n",
      "klo\n",
      "gak\n",
      "cuma\n",
      "doang\n",
      "kayak\n",
      "memek\n",
      "asu\n",
      "babi\n",
      "kecil\n",
      "v\n",
      "tapi\n",
      "ngakak\n",
      "ericko\n",
      "lim\n",
      "walaupun\n",
      "toxic\n",
      "mantap\n",
      "dah\n",
      "rik\n",
      "tuh\n",
      "aja\n",
      "gua\n",
      "konten\n",
      "adsense\n",
      "masih\n",
      "iya\n",
      "banget\n",
      "anjeng\n",
      "bikin\n",
      "vidio\n",
      "ada\n",
      "yg\n",
      "kali\n",
      "dari\n",
      "goblok\n",
      "ni\n",
      "bgt\n",
      "kek\n",
      "org\n",
      "subscribe\n",
      "trus\n",
      "komentar\n",
      "lagi\n",
      "biar\n",
      "hater\n",
      "kagak\n",
      "terus\n",
      "harus\n",
      "semangat\n",
      "aku\n",
      "tau\n",
      "terkenal\n",
      "bukan\n",
      "hal\n",
      "yang\n",
      "dan\n",
      "tetap\n",
      "eric\n",
      "jadi\n",
      "youtuber\n",
      "lebih\n",
      "sukses\n",
      "bisa\n",
      "erick\n",
      "fans\n",
      "karna\n",
      "kalau\n",
      "jangan\n",
      "bau\n",
      "karya\n",
      "sabar\n",
      "ngomong\n",
      "usah\n",
      "i\n",
      "love\n",
      "you\n",
      "auto\n",
      "subs\n",
      "like\n",
      "bilang\n",
      "sama\n",
      "orang\n",
      "jembut\n",
      "otak\n",
      "bg\n",
      "para\n",
      "tu\n",
      "banyak\n",
      "bro\n",
      "menghibur\n",
      "anti\n",
      "si\n",
      "mah\n",
      "k\n",
      "ajg\n",
      "wkwk\n",
      "nya\n",
      "tetep\n",
      "salah\n",
      "ganteng\n",
      "penting\n",
      "berkarya\n",
      "comment\n",
      "intro\n",
      "emang\n",
      "channel\n",
      "tanpa\n",
      "bakal\n",
      "ini\n",
      "hahaha\n",
      "heters\n",
      "tolol\n",
      "hiburan\n",
      "mendidik\n",
      "is\n",
      "the\n",
      "duit\n",
      "btw\n",
      "d\n",
      "jgn\n",
      "tpi\n",
      "kan\n",
      "bocah\n",
      "njing\n",
      "satu\n",
      "mati\n",
      "kapan\n",
      "komen\n",
      "bangsat\n",
      "eh\n",
      "subscriber\n",
      "atau\n",
      "lain\n",
      "sangat\n",
      "karena\n",
      "bego\n",
      "punya\n",
      "mending\n",
      "kyk\n",
      "gini\n",
      "kasar\n",
      "sumpah\n",
      "gila\n",
      "pada\n",
      "koment\n",
      "lah\n",
      "erickontol\n",
      "baru\n",
      "kocak\n",
      "pertama\n",
      "belum\n",
      "manis\n",
      "ah\n",
      "elu\n",
      "tai\n",
      "juta\n",
      "pake\n",
      "a\n",
      "good\n",
      "gimana\n",
      "coli\n",
      "pasti\n",
      "makin\n",
      "kaya\n",
      "amat\n",
      "ama\n",
      "setiap\n",
      "ngehate\n",
      "main\n",
      "udah\n",
      "tp\n",
      "mereka\n",
      "bener\n",
      "sih\n",
      "y\n",
      "liat\n",
      "g\n",
      "wkwkwk\n",
      "coment\n",
      "pas\n",
      "paling\n",
      "hidup\n",
      "hanya\n",
      "semoga\n",
      "kalian\n",
      "dapet\n",
      "wkwkwkwk\n",
      "untuk\n",
      "mana\n",
      "gitu\n",
      "boong\n",
      "bagi\n",
      "diri\n",
      "sendiri\n",
      "bacot\n",
      "ngentot\n",
      "x\n",
      "kok\n",
      "ngilu\n",
      "setuju\n",
      "and\n",
      "iri\n",
      "ric\n",
      "minta\n",
      "mirip\n",
      "dengan\n",
      "coba\n",
      "lucu\n",
      "yt\n",
      "malah\n",
      "ketawa\n",
      "tonton\n",
      "dong\n",
      "erik\n",
      "so\n",
      "keren\n",
      "jdi\n",
      "jelek\n",
      "uang\n",
      "dasar\n",
      "ga\n",
      "anjir\n",
      "berfaedah\n",
      "makan\n",
      "berarti\n",
      "kenapa\n",
      "saya\n",
      "kak\n",
      "masuk\n",
      "dukung\n",
      "cuman\n",
      "kntl\n",
      "woy\n",
      "u\n",
      "pernah\n",
      "wkwkw\n",
      "arap\n",
      "soapers\n",
      "menit\n",
      "ke\n",
      "cara\n",
      "beda\n",
      "mungkin\n",
      "lg\n",
      "baik\n",
      "bagus\n",
      "mau\n",
      "bully\n",
      "seneng\n",
      "chanel\n",
      "anjg\n",
      "w\n",
      "gara\n",
      "ngentod\n",
      "erikontol\n",
      "lama\n",
      "hatters\n",
      "luh\n",
      "loh\n",
      "l\n",
      "tiap\n",
      "hari\n",
      "sampah\n",
      "salam\n",
      "sekarang\n",
      "hate\n",
      "kamu\n",
      "yaa\n",
      "pendidikan\n",
      "langsung\n",
      "videonya\n",
      "menurut\n",
      "soapking\n",
      "best\n",
      "t\n",
      "indonesia\n",
      "aj\n",
      "gausah\n",
      "fuck\n",
      "temen\n",
      "sampe\n",
      "upload\n",
      "erico\n",
      "no\n",
      "limit\n",
      "woi\n",
      "kita\n",
      "tidak\n",
      "beli\n",
      "skip\n",
      "anjink\n",
      "cari\n",
      "udh\n",
      "kaga\n",
      "saran\n",
      "in\n",
      "njir\n",
      "kata\n",
      "sekali\n",
      "tambah\n",
      "baca\n",
      "kurang\n",
      "jujur\n",
      "mikir\n",
      "rewind\n",
      "benci\n",
      "abang\n",
      "nge\n",
      "reza\n",
      "akan\n",
      "manusia\n",
      "kasih\n",
      "makanya\n",
      "nggak\n",
      "terhibur\n",
      "ea\n",
      "padahal\n",
      "sering\n",
      "ambil\n",
      "emng\n",
      "agama\n",
      "friendly\n",
      "kebanyakan\n",
      "bgst\n",
      "tolong\n",
      "ngatain\n",
      "dunia\n",
      "hati\n",
      "jt\n",
      "yah\n",
      "m\n",
      "jahat\n",
      "namanya\n",
      "selalu\n",
      "nih\n",
      "hapus\n",
      "anda\n",
      "adalah\n",
      "oh\n",
      "kau\n",
      "tod\n",
      "loe\n",
      "jelas\n",
      "lagu\n",
      "boleh\n",
      "sini\n",
      "family\n",
      "wik\n",
      "support\n",
      "siapa\n",
      "gblk\n",
      "heaters\n",
      "hiya\n",
      "dulu\n",
      "dislike\n",
      "opening\n",
      "akhirnya\n",
      "botak\n",
      "pengen\n",
      "dengerin\n",
      "an\n",
      "dalam\n",
      "ngga\n",
      "nama\n",
      "biasa\n",
      "qorygore\n",
      "iklan\n",
      "back\n",
      "ko\n",
      "iklannya\n",
      "tung\n",
      "legend\n",
      "{'rick': 0, 'lu': 1, 'anak': 2, 'ya': 3, 'itu': 4, 'bang': 5, 'gw': 6, 'sub': 7, 'kontol': 8, 'apa': 9, 'di': 10, 'mu': 11, 'muka': 12, 'lo': 13, 'anjing': 14, 'gue': 15, 'nonton': 16, 'youtube': 17, 'suka': 18, 'gaya': 19, 'haha': 20, 'wkwkwkw': 21, 'buat': 22, 'haters': 23, 'semua': 24, 'sok': 25, 'video': 26, 'erickolim': 27, 'kalo': 28, 'gk': 29, 'ngapain': 30, 'ditonton': 31, 'eriko': 32, 'juga': 33, 'dia': 34, 'klo': 35, 'gak': 36, 'cuma': 37, 'doang': 38, 'kayak': 39, 'memek': 40, 'asu': 41, 'babi': 42, 'kecil': 43, 'v': 44, 'tapi': 45, 'ngakak': 46, 'ericko': 47, 'lim': 48, 'walaupun': 49, 'toxic': 50, 'mantap': 51, 'dah': 52, 'rik': 53, 'tuh': 54, 'aja': 55, 'gua': 56, 'konten': 57, 'adsense': 58, 'masih': 59, 'iya': 60, 'banget': 61, 'anjeng': 62, 'bikin': 63, 'vidio': 64, 'ada': 65, 'yg': 66, 'kali': 67, 'dari': 68, 'goblok': 69, 'ni': 70, 'bgt': 71, 'kek': 72, 'org': 73, 'subscribe': 74, 'trus': 75, 'komentar': 76, 'lagi': 77, 'biar': 78, 'hater': 79, 'kagak': 80, 'terus': 81, 'harus': 82, 'semangat': 83, 'aku': 84, 'tau': 85, 'terkenal': 86, 'bukan': 87, 'hal': 88, 'yang': 89, 'dan': 90, 'tetap': 91, 'eric': 92, 'jadi': 93, 'youtuber': 94, 'lebih': 95, 'sukses': 96, 'bisa': 97, 'erick': 98, 'fans': 99, 'karna': 100, 'kalau': 101, 'jangan': 102, 'bau': 103, 'karya': 104, 'sabar': 105, 'ngomong': 106, 'usah': 107, 'i': 108, 'love': 109, 'you': 110, 'auto': 111, 'subs': 112, 'like': 113, 'bilang': 114, 'sama': 115, 'orang': 116, 'jembut': 117, 'otak': 118, 'bg': 119, 'para': 120, 'tu': 121, 'banyak': 122, 'bro': 123, 'menghibur': 124, 'anti': 125, 'si': 126, 'mah': 127, 'k': 128, 'ajg': 129, 'wkwk': 130, 'nya': 131, 'tetep': 132, 'salah': 133, 'ganteng': 134, 'penting': 135, 'berkarya': 136, 'comment': 137, 'intro': 138, 'emang': 139, 'channel': 140, 'tanpa': 141, 'bakal': 142, 'ini': 143, 'hahaha': 144, 'heters': 145, 'tolol': 146, 'hiburan': 147, 'mendidik': 148, 'is': 149, 'the': 150, 'duit': 151, 'btw': 152, 'd': 153, 'jgn': 154, 'tpi': 155, 'kan': 156, 'bocah': 157, 'njing': 158, 'satu': 159, 'mati': 160, 'kapan': 161, 'komen': 162, 'bangsat': 163, 'eh': 164, 'subscriber': 165, 'atau': 166, 'lain': 167, 'sangat': 168, 'karena': 169, 'bego': 170, 'punya': 171, 'mending': 172, 'kyk': 173, 'gini': 174, 'kasar': 175, 'sumpah': 176, 'gila': 177, 'pada': 178, 'koment': 179, 'lah': 180, 'erickontol': 181, 'baru': 182, 'kocak': 183, 'pertama': 184, 'belum': 185, 'manis': 186, 'ah': 187, 'elu': 188, 'tai': 189, 'juta': 190, 'pake': 191, 'a': 192, 'good': 193, 'gimana': 194, 'coli': 195, 'pasti': 196, 'makin': 197, 'kaya': 198, 'amat': 199, 'ama': 200, 'setiap': 201, 'ngehate': 202, 'main': 203, 'udah': 204, 'tp': 205, 'mereka': 206, 'bener': 207, 'sih': 208, 'y': 209, 'liat': 210, 'g': 211, 'wkwkwk': 212, 'coment': 213, 'pas': 214, 'paling': 215, 'hidup': 216, 'hanya': 217, 'semoga': 218, 'kalian': 219, 'dapet': 220, 'wkwkwkwk': 221, 'untuk': 222, 'mana': 223, 'gitu': 224, 'boong': 225, 'bagi': 226, 'diri': 227, 'sendiri': 228, 'bacot': 229, 'ngentot': 230, 'x': 231, 'kok': 232, 'ngilu': 233, 'setuju': 234, 'and': 235, 'iri': 236, 'ric': 237, 'minta': 238, 'mirip': 239, 'dengan': 240, 'coba': 241, 'lucu': 242, 'yt': 243, 'malah': 244, 'ketawa': 245, 'tonton': 246, 'dong': 247, 'erik': 248, 'so': 249, 'keren': 250, 'jdi': 251, 'jelek': 252, 'uang': 253, 'dasar': 254, 'ga': 255, 'anjir': 256, 'berfaedah': 257, 'makan': 258, 'berarti': 259, 'kenapa': 260, 'saya': 261, 'kak': 262, 'masuk': 263, 'dukung': 264, 'cuman': 265, 'kntl': 266, 'woy': 267, 'u': 268, 'pernah': 269, 'wkwkw': 270, 'arap': 271, 'soapers': 272, 'menit': 273, 'ke': 274, 'cara': 275, 'beda': 276, 'mungkin': 277, 'lg': 278, 'baik': 279, 'bagus': 280, 'mau': 281, 'bully': 282, 'seneng': 283, 'chanel': 284, 'anjg': 285, 'w': 286, 'gara': 287, 'ngentod': 288, 'erikontol': 289, 'lama': 290, 'hatters': 291, 'luh': 292, 'loh': 293, 'l': 294, 'tiap': 295, 'hari': 296, 'sampah': 297, 'salam': 298, 'sekarang': 299, 'hate': 300, 'kamu': 301, 'yaa': 302, 'pendidikan': 303, 'langsung': 304, 'videonya': 305, 'menurut': 306, 'soapking': 307, 'best': 308, 't': 309, 'indonesia': 310, 'aj': 311, 'gausah': 312, 'fuck': 313, 'temen': 314, 'sampe': 315, 'upload': 316, 'erico': 317, 'no': 318, 'limit': 319, 'woi': 320, 'kita': 321, 'tidak': 322, 'beli': 323, 'skip': 324, 'anjink': 325, 'cari': 326, 'udh': 327, 'kaga': 328, 'saran': 329, 'in': 330, 'njir': 331, 'kata': 332, 'sekali': 333, 'tambah': 334, 'baca': 335, 'kurang': 336, 'jujur': 337, 'mikir': 338, 'rewind': 339, 'benci': 340, 'abang': 341, 'nge': 342, 'reza': 343, 'akan': 344, 'manusia': 345, 'kasih': 346, 'makanya': 347, 'nggak': 348, 'terhibur': 349, 'ea': 350, 'padahal': 351, 'sering': 352, 'ambil': 353, 'emng': 354, 'agama': 355, 'friendly': 356, 'kebanyakan': 357, 'bgst': 358, 'tolong': 359, 'ngatain': 360, 'dunia': 361, 'hati': 362, 'jt': 363, 'yah': 364, 'm': 365, 'jahat': 366, 'namanya': 367, 'selalu': 368, 'nih': 369, 'hapus': 370, 'anda': 371, 'adalah': 372, 'oh': 373, 'kau': 374, 'tod': 375, 'loe': 376, 'jelas': 377, 'lagu': 378, 'boleh': 379, 'sini': 380, 'family': 381, 'wik': 382, 'support': 383, 'siapa': 384, 'gblk': 385, 'heaters': 386, 'hiya': 387, 'dulu': 388, 'dislike': 389, 'opening': 390, 'akhirnya': 391, 'botak': 392, 'pengen': 393, 'dengerin': 394, 'an': 395, 'dalam': 396, 'ngga': 397, 'nama': 398, 'biasa': 399, 'qorygore': 400, 'iklan': 401, 'back': 402, 'ko': 403, 'iklannya': 404, 'tung': 405, 'legend': 406}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as fun\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def create_vocab(corpus, limit_occ = 50):\n",
    "    \"\"\"\n",
    "    membuat vocabulary, dengan 2 info penting: \n",
    "        1. word_id -> mapping kata ke id (int)\n",
    "        2. id_word -> sebaliknya, mapping dari id ke word\n",
    "    \"\"\"\n",
    "    word_id = {}\n",
    "    id_word = []\n",
    "\n",
    "    occurence = dict()\n",
    "    i = 0\n",
    "    for sentence in corpus:\n",
    "        for word in sentence:\n",
    "            occurence.setdefault(word, 0)\n",
    "            occurence[word] += 1\n",
    "            \n",
    "    for k, v in occurence.items():\n",
    "        if(v < limit_occ): continue\n",
    "        print(k)\n",
    "        word_id[k] = i\n",
    "        id_word.append(k)\n",
    "        i += 1\n",
    "                \n",
    "    id_word = {id: word for (word, id) in word_id.items()}\n",
    "    return word_id, id_word\n",
    "\n",
    "\n",
    "word_id, id_word = create_vocab(corpus)\n",
    "vocab_size = len(word_id)\n",
    "\n",
    "print(word_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset generator\n",
    "def iter_dataset(corpus, context_window=1):\n",
    "    for sentence in corpus:\n",
    "        words = sentence\n",
    "        for i in range(len(words)):\n",
    "            center_word = words[i]\n",
    "            # cari konteks ke kiri\n",
    "            j = i - 1\n",
    "            while (j >= i - context_window) and (j >= 0):\n",
    "                context_word = words[j]\n",
    "                yield (center_word, context_word)\n",
    "                j -= 1\n",
    "            # cari konteks ke kanan\n",
    "            j = i + 1\n",
    "            while (j <= i + context_window) and (j < len(words)):\n",
    "                context_word = words[j]\n",
    "                yield (center_word, context_word)\n",
    "                j += 1\n",
    "\n",
    "\n",
    "def get_dataset(corpus):\n",
    "    # membuat dataset\n",
    "    X = []\n",
    "    Y = []\n",
    "    for center, context in iter_dataset(corpus):\n",
    "        try:\n",
    "            if(center not in word_id.keys() or context not in word_id.keys()):\n",
    "                continue\n",
    "            X.append(word_id[center])\n",
    "            Y.append(word_id[context])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    X_ctr = torch.tensor(X)\n",
    "    Y_ctx = torch.tensor(Y)\n",
    "    return X_ctr, Y_ctx\n",
    "\n",
    "\n",
    "def get_input_tensor(tensor):\n",
    "    return fun.one_hot(tensor, num_classes=vocab_size)\n",
    "\n",
    "\n",
    "center_list, context_list = get_dataset(corpus)\n",
    "# get_input_tensor(center_list[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax\n",
    "def softmax(x):\n",
    "    maxes = torch.max(x, 1, keepdim=True)[0]\n",
    "    x_exp = torch.exp(x - maxes)\n",
    "    x_exp_sum = torch.sum(x_exp, 1, keepdim=True)\n",
    "    return x_exp / x_exp_sum\n",
    "\n",
    "# Categorical Cross Entropy Loss\n",
    "\n",
    "\n",
    "def CCEloss(Y_pred, Y_true):\n",
    "    m = Y_pred.size()[0]\n",
    "    return -(1 / m) * torch.sum(Y_true * torch.log(Y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_c = None\n",
    "tmp_w = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(X_ctr, Y_ctx, EPOCHS = 200, load = False):\n",
    "    global tmp_w\n",
    "    global tmp_c\n",
    "\n",
    "    EMBEDDING_DIMS = 10\n",
    "\n",
    "    initrange = 1\n",
    "\n",
    "    # trick: di awal, mode gradient jangan diaktifkan dahulu, karena ingin\n",
    "    # memodifikasi nilai di C dan W secara in-place\n",
    "    C = torch.rand(vocab_size, EMBEDDING_DIMS, requires_grad = False)\n",
    "    W = torch.rand(EMBEDDING_DIMS, vocab_size, requires_grad = False)\n",
    "\n",
    "    # agar nilai awal parameter di C dan W berkisar di antara -initrange hingga +initrange\n",
    "    C = -2 * initrange * C + initrange\n",
    "    W = -2 * initrange * W + initrange\n",
    "\n",
    "    # setelah operasi in-place modification, baru kita set mode gradient\n",
    "    C.requires_grad = True\n",
    "    W.requires_grad = True\n",
    "    if(load):\n",
    "        C = tmp_c\n",
    "        W = tmp_w\n",
    "\n",
    "    print(f'C shape is: {C.shape}, W shape is: {W.shape}')\n",
    "    print(C)\n",
    "    print(W)\n",
    "\n",
    "    LEARNING_RATE = 0.2\n",
    "    LR_DECAY = 0.99\n",
    "\n",
    "    for i in range(EPOCHS):\n",
    "        X = get_input_tensor(X_ctr).float()\n",
    "        Y = get_input_tensor(Y_ctx).float()\n",
    "\n",
    "        h = X.mm(C)\n",
    "        Y_pred = softmax(h.mm(W))\n",
    "\n",
    "        loss = CCEloss(Y_pred, Y)\n",
    "        loss.backward()\n",
    "\n",
    "        # update C dan W\n",
    "        with torch.no_grad():\n",
    "            C -= LEARNING_RATE * C.grad\n",
    "            W -= LEARNING_RATE * W.grad\n",
    "            \n",
    "            C.grad.zero_()\n",
    "            W.grad.zero_()\n",
    "        tmp_w = W\n",
    "        tmp_c = C\n",
    "        if i % 10 == 0:\n",
    "            # kita coba lihat progress cosine similarity\n",
    "            # antara embedding \"lo\" dengan \"lu\"\n",
    "            vector_1 = C[word_id[\"lo\"]]\n",
    "            vector_2 = C[word_id[\"lu\"]]\n",
    "            sim = fun.cosine_similarity(vector_1, vector_2, dim = 0)\n",
    "            print(f'Epoch {i}, loss = {loss}, sim(lo, lu) = {sim}')\n",
    "    return C, W\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C shape is: torch.Size([407, 10]), W shape is: torch.Size([10, 407])\n",
      "tensor([[ 0.7699,  0.0689,  0.0686,  ...,  0.1680, -0.8254,  0.0960],\n",
      "        [-0.1037,  0.8447,  0.5698,  ...,  0.9096,  0.2174, -0.7846],\n",
      "        [-0.7678, -0.9936, -0.3147,  ..., -0.8590,  0.3445, -0.2095],\n",
      "        ...,\n",
      "        [-0.0073, -0.4168, -0.1123,  ...,  0.3423, -0.5214,  0.5765],\n",
      "        [ 0.3406, -0.9154,  0.9959,  ...,  0.3365,  0.4900, -0.7793],\n",
      "        [ 0.7294,  0.0520,  0.5655,  ..., -0.0241,  0.1011,  0.8905]],\n",
      "       requires_grad=True)\n",
      "tensor([[-0.7078, -0.3604,  0.6459,  ..., -0.6949,  0.2670, -0.3616],\n",
      "        [-0.7790,  0.5272,  0.4276,  ..., -0.8605, -0.7306, -0.6701],\n",
      "        [-0.8267,  0.8954, -0.2884,  ...,  0.4384, -0.0119,  0.5845],\n",
      "        ...,\n",
      "        [-0.5367, -0.4530, -0.4226,  ...,  0.5493,  0.1827,  0.6038],\n",
      "        [-0.6155, -0.9396,  0.1070,  ...,  0.1873, -0.7378, -0.3369],\n",
      "        [-0.8514,  0.1236, -0.6284,  ...,  0.7436,  0.3376, -0.7323]],\n",
      "       requires_grad=True)\n",
      "Epoch 0, loss = 6.424861431121826, sim(lo, lu) = -0.34034988284111023\n",
      "Epoch 10, loss = 6.3736748695373535, sim(lo, lu) = -0.34059420228004456\n",
      "Epoch 20, loss = 6.321953296661377, sim(lo, lu) = -0.34077584743499756\n",
      "Epoch 30, loss = 6.271151065826416, sim(lo, lu) = -0.340892493724823\n",
      "Epoch 40, loss = 6.225174427032471, sim(lo, lu) = -0.34094148874282837\n",
      "Epoch 50, loss = 6.188357830047607, sim(lo, lu) = -0.34091997146606445\n",
      "Epoch 60, loss = 6.1611714363098145, sim(lo, lu) = -0.34082531929016113\n",
      "Epoch 70, loss = 6.140697956085205, sim(lo, lu) = -0.3406548500061035\n",
      "Epoch 80, loss = 6.124091148376465, sim(lo, lu) = -0.34040582180023193\n",
      "Epoch 90, loss = 6.109654426574707, sim(lo, lu) = -0.34007543325424194\n",
      "Epoch 100, loss = 6.096508026123047, sim(lo, lu) = -0.3396611213684082\n",
      "Epoch 110, loss = 6.084188938140869, sim(lo, lu) = -0.3391597867012024\n",
      "Epoch 120, loss = 6.072443008422852, sim(lo, lu) = -0.33856862783432007\n",
      "Epoch 130, loss = 6.061119079589844, sim(lo, lu) = -0.3378845155239105\n",
      "Epoch 140, loss = 6.050127983093262, sim(lo, lu) = -0.3371046781539917\n",
      "Epoch 150, loss = 6.039407253265381, sim(lo, lu) = -0.33622586727142334\n",
      "Epoch 160, loss = 6.0289130210876465, sim(lo, lu) = -0.33524519205093384\n",
      "Epoch 170, loss = 6.018615245819092, sim(lo, lu) = -0.33415964245796204\n",
      "Epoch 180, loss = 6.008491516113281, sim(lo, lu) = -0.33296623826026917\n",
      "Epoch 190, loss = 5.998522758483887, sim(lo, lu) = -0.33166196942329407\n",
      "Epoch 200, loss = 5.988696575164795, sim(lo, lu) = -0.3302440345287323\n",
      "Epoch 210, loss = 5.979001998901367, sim(lo, lu) = -0.32870933413505554\n",
      "Epoch 220, loss = 5.969429969787598, sim(lo, lu) = -0.3270552456378937\n",
      "Epoch 230, loss = 5.959975719451904, sim(lo, lu) = -0.3252788484096527\n",
      "Epoch 240, loss = 5.9506378173828125, sim(lo, lu) = -0.3233775794506073\n",
      "Epoch 250, loss = 5.9414143562316895, sim(lo, lu) = -0.32134875655174255\n",
      "Epoch 260, loss = 5.932309150695801, sim(lo, lu) = -0.3191896378993988\n",
      "Epoch 270, loss = 5.9233245849609375, sim(lo, lu) = -0.31689783930778503\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m embedding_C, context_W \u001b[39m=\u001b[39m train(center_list, context_list, \u001b[39m500\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m)\n",
      "Cell \u001b[0;32mIn [22], line 40\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(X_ctr, Y_ctx, EPOCHS, load)\u001b[0m\n\u001b[1;32m     37\u001b[0m Y_pred \u001b[39m=\u001b[39m softmax(h\u001b[39m.\u001b[39mmm(W))\n\u001b[1;32m     39\u001b[0m loss \u001b[39m=\u001b[39m CCEloss(Y_pred, Y)\n\u001b[0;32m---> 40\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     42\u001b[0m \u001b[39m# update C dan W\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.7/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.7/lib/python3.9/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "embedding_C, context_W = train(center_list, context_list, 500, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C shape is: torch.Size([407, 10]), W shape is: torch.Size([10, 407])\n",
      "tensor([[ 0.5541,  0.4224,  0.2032,  ...,  0.3763, -0.9508, -0.0233],\n",
      "        [ 0.0225,  0.4562,  0.2719,  ...,  0.6570, -0.4622, -0.8025],\n",
      "        [-0.2941, -0.5128, -0.0827,  ..., -0.4481,  0.2397, -0.3369],\n",
      "        ...,\n",
      "        [-0.0154, -0.3424, -0.0734,  ...,  0.3606, -0.4895,  0.5099],\n",
      "        [ 0.8068, -2.2320,  1.3647,  ...,  0.6656, -0.1872, -0.6056],\n",
      "        [ 0.7607, -0.0101,  0.5153,  ..., -0.0170,  0.1398,  0.7353]],\n",
      "       requires_grad=True)\n",
      "tensor([[-0.4861,  0.7008,  0.5579,  ..., -0.6535,  0.6670, -0.2758],\n",
      "        [-0.4886,  1.7876,  0.3512,  ..., -0.7703, -2.1920, -0.5970],\n",
      "        [-0.5341,  0.8421, -0.2261,  ...,  0.2875,  0.9685,  0.2870],\n",
      "        ...,\n",
      "        [-0.1760,  0.4762, -0.5302,  ...,  0.4241,  0.4319,  0.4583],\n",
      "        [-0.6933, -2.3615,  0.1322,  ...,  0.2927, -0.2280, -0.2128],\n",
      "        [-0.8266,  0.3524, -0.6373,  ...,  0.6685, -0.2907, -0.5295]],\n",
      "       requires_grad=True)\n",
      "Epoch 0, loss = 5.225616455078125, sim(lo, lu) = 0.9535356760025024\n",
      "Epoch 10, loss = 5.223560810089111, sim(lo, lu) = 0.9543201923370361\n",
      "Epoch 20, loss = 5.221512794494629, sim(lo, lu) = 0.9550849795341492\n",
      "Epoch 30, loss = 5.219473361968994, sim(lo, lu) = 0.9558307528495789\n",
      "Epoch 40, loss = 5.217442989349365, sim(lo, lu) = 0.956558108329773\n",
      "Epoch 50, loss = 5.215421199798584, sim(lo, lu) = 0.9572674036026001\n",
      "Epoch 60, loss = 5.213407039642334, sim(lo, lu) = 0.9579591751098633\n",
      "Epoch 70, loss = 5.21140193939209, sim(lo, lu) = 0.958634078502655\n",
      "Epoch 80, loss = 5.209404945373535, sim(lo, lu) = 0.9592924118041992\n",
      "Epoch 90, loss = 5.2074174880981445, sim(lo, lu) = 0.959934651851654\n",
      "Epoch 100, loss = 5.205437660217285, sim(lo, lu) = 0.9605611562728882\n",
      "Epoch 110, loss = 5.203466415405273, sim(lo, lu) = 0.9611726403236389\n",
      "Epoch 120, loss = 5.201503753662109, sim(lo, lu) = 0.9617690443992615\n",
      "Epoch 130, loss = 5.199549674987793, sim(lo, lu) = 0.9623511433601379\n",
      "Epoch 140, loss = 5.197604179382324, sim(lo, lu) = 0.9629192352294922\n",
      "Epoch 150, loss = 5.195666313171387, sim(lo, lu) = 0.9634737968444824\n",
      "Epoch 160, loss = 5.193737506866455, sim(lo, lu) = 0.9640151262283325\n",
      "Epoch 170, loss = 5.191816806793213, sim(lo, lu) = 0.9645432829856873\n",
      "Epoch 180, loss = 5.189905166625977, sim(lo, lu) = 0.9650591015815735\n",
      "Epoch 190, loss = 5.1880011558532715, sim(lo, lu) = 0.9655625820159912\n",
      "Epoch 200, loss = 5.186105728149414, sim(lo, lu) = 0.9660542607307434\n",
      "Epoch 210, loss = 5.184218883514404, sim(lo, lu) = 0.9665342569351196\n",
      "Epoch 220, loss = 5.182340621948242, sim(lo, lu) = 0.9670027494430542\n",
      "Epoch 230, loss = 5.180469512939453, sim(lo, lu) = 0.9674606323242188\n",
      "Epoch 240, loss = 5.178607940673828, sim(lo, lu) = 0.9679076671600342\n",
      "Epoch 250, loss = 5.176753520965576, sim(lo, lu) = 0.9683442115783691\n",
      "Epoch 260, loss = 5.174908638000488, sim(lo, lu) = 0.9687707424163818\n",
      "Epoch 270, loss = 5.173070907592773, sim(lo, lu) = 0.9691874384880066\n",
      "Epoch 280, loss = 5.1712422370910645, sim(lo, lu) = 0.9695943593978882\n",
      "Epoch 290, loss = 5.169421195983887, sim(lo, lu) = 0.9699919819831848\n",
      "Epoch 300, loss = 5.167608261108398, sim(lo, lu) = 0.970380425453186\n",
      "Epoch 310, loss = 5.165803909301758, sim(lo, lu) = 0.9707601070404053\n",
      "Epoch 320, loss = 5.164008140563965, sim(lo, lu) = 0.9711310863494873\n",
      "Epoch 330, loss = 5.162219047546387, sim(lo, lu) = 0.9714934229850769\n",
      "Epoch 340, loss = 5.160438537597656, sim(lo, lu) = 0.9718477129936218\n",
      "Epoch 350, loss = 5.158666610717773, sim(lo, lu) = 0.9721943736076355\n",
      "Epoch 360, loss = 5.156901836395264, sim(lo, lu) = 0.9725328087806702\n",
      "Epoch 370, loss = 5.15514612197876, sim(lo, lu) = 0.9728636741638184\n",
      "Epoch 380, loss = 5.153397083282471, sim(lo, lu) = 0.9731872081756592\n",
      "Epoch 390, loss = 5.1516571044921875, sim(lo, lu) = 0.9735035300254822\n",
      "Epoch 400, loss = 5.149924278259277, sim(lo, lu) = 0.9738129377365112\n",
      "Epoch 410, loss = 5.148200035095215, sim(lo, lu) = 0.974115252494812\n",
      "Epoch 420, loss = 5.146482944488525, sim(lo, lu) = 0.9744112491607666\n",
      "Epoch 430, loss = 5.144773960113525, sim(lo, lu) = 0.9747005701065063\n",
      "Epoch 440, loss = 5.143073081970215, sim(lo, lu) = 0.9749833345413208\n",
      "Epoch 450, loss = 5.1413798332214355, sim(lo, lu) = 0.9752603769302368\n",
      "Epoch 460, loss = 5.139694690704346, sim(lo, lu) = 0.9755310416221619\n",
      "Epoch 470, loss = 5.138016223907471, sim(lo, lu) = 0.9757958054542542\n",
      "Epoch 480, loss = 5.136346340179443, sim(lo, lu) = 0.9760552644729614\n",
      "Epoch 490, loss = 5.134684085845947, sim(lo, lu) = 0.9763088822364807\n",
      "Epoch 500, loss = 5.133028984069824, sim(lo, lu) = 0.9765570759773254\n",
      "Epoch 510, loss = 5.131381511688232, sim(lo, lu) = 0.9767997860908508\n",
      "Epoch 520, loss = 5.129741191864014, sim(lo, lu) = 0.9770376086235046\n",
      "Epoch 530, loss = 5.128108978271484, sim(lo, lu) = 0.9772701859474182\n",
      "Epoch 540, loss = 5.1264848709106445, sim(lo, lu) = 0.9774982929229736\n",
      "Epoch 550, loss = 5.124868392944336, sim(lo, lu) = 0.9777213335037231\n",
      "Epoch 560, loss = 5.123258590698242, sim(lo, lu) = 0.9779394865036011\n",
      "Epoch 570, loss = 5.121654987335205, sim(lo, lu) = 0.9781532883644104\n",
      "Epoch 580, loss = 5.120060920715332, sim(lo, lu) = 0.9783628582954407\n",
      "Epoch 590, loss = 5.118473052978516, sim(lo, lu) = 0.978567898273468\n",
      "Epoch 600, loss = 5.116893291473389, sim(lo, lu) = 0.9787686467170715\n",
      "Epoch 610, loss = 5.115320205688477, sim(lo, lu) = 0.9789655208587646\n",
      "Epoch 620, loss = 5.1137542724609375, sim(lo, lu) = 0.9791582822799683\n",
      "Epoch 630, loss = 5.11219596862793, sim(lo, lu) = 0.9793470501899719\n",
      "Epoch 640, loss = 5.110644340515137, sim(lo, lu) = 0.97953200340271\n",
      "Epoch 650, loss = 5.109100341796875, sim(lo, lu) = 0.9797133207321167\n",
      "Epoch 660, loss = 5.1075639724731445, sim(lo, lu) = 0.9798909425735474\n",
      "Epoch 670, loss = 5.106033802032471, sim(lo, lu) = 0.9800652265548706\n",
      "Epoch 680, loss = 5.10451078414917, sim(lo, lu) = 0.9802359342575073\n",
      "Epoch 690, loss = 5.102995872497559, sim(lo, lu) = 0.9804031252861023\n",
      "Epoch 700, loss = 5.101486682891846, sim(lo, lu) = 0.980567216873169\n",
      "Epoch 710, loss = 5.099985122680664, sim(lo, lu) = 0.9807280898094177\n",
      "Epoch 720, loss = 5.0984907150268555, sim(lo, lu) = 0.9808858633041382\n",
      "Epoch 730, loss = 5.097002983093262, sim(lo, lu) = 0.9810402989387512\n",
      "Epoch 740, loss = 5.095521450042725, sim(lo, lu) = 0.9811919927597046\n",
      "Epoch 750, loss = 5.094048023223877, sim(lo, lu) = 0.9813408255577087\n",
      "Epoch 760, loss = 5.092580795288086, sim(lo, lu) = 0.9814868569374084\n",
      "Epoch 770, loss = 5.09112024307251, sim(lo, lu) = 0.9816299080848694\n",
      "Epoch 780, loss = 5.089666843414307, sim(lo, lu) = 0.9817705750465393\n",
      "Epoch 790, loss = 5.088220119476318, sim(lo, lu) = 0.9819083213806152\n",
      "Epoch 800, loss = 5.086780071258545, sim(lo, lu) = 0.9820436239242554\n",
      "Epoch 810, loss = 5.085346698760986, sim(lo, lu) = 0.9821763038635254\n",
      "Epoch 820, loss = 5.083920001983643, sim(lo, lu) = 0.982306718826294\n",
      "Epoch 830, loss = 5.082499980926514, sim(lo, lu) = 0.9824346899986267\n",
      "Epoch 840, loss = 5.081086158752441, sim(lo, lu) = 0.982560396194458\n",
      "Epoch 850, loss = 5.079679489135742, sim(lo, lu) = 0.9826836585998535\n",
      "Epoch 860, loss = 5.0782790184021, sim(lo, lu) = 0.9828048348426819\n",
      "Epoch 870, loss = 5.07688570022583, sim(lo, lu) = 0.9829236268997192\n",
      "Epoch 880, loss = 5.075497150421143, sim(lo, lu) = 0.9830403923988342\n",
      "Epoch 890, loss = 5.0741167068481445, sim(lo, lu) = 0.9831554889678955\n",
      "Epoch 900, loss = 5.072741985321045, sim(lo, lu) = 0.9832679033279419\n",
      "Epoch 910, loss = 5.07137393951416, sim(lo, lu) = 0.9833788275718689\n",
      "Epoch 920, loss = 5.070011615753174, sim(lo, lu) = 0.9834878444671631\n",
      "Epoch 930, loss = 5.0686564445495605, sim(lo, lu) = 0.9835947155952454\n",
      "Epoch 940, loss = 5.067307949066162, sim(lo, lu) = 0.9836999177932739\n",
      "Epoch 950, loss = 5.065964221954346, sim(lo, lu) = 0.9838030338287354\n",
      "Epoch 960, loss = 5.0646281242370605, sim(lo, lu) = 0.9839048385620117\n",
      "Epoch 970, loss = 5.063297748565674, sim(lo, lu) = 0.9840048551559448\n",
      "Epoch 980, loss = 5.0619730949401855, sim(lo, lu) = 0.9841030836105347\n",
      "Epoch 990, loss = 5.060654640197754, sim(lo, lu) = 0.984199583530426\n"
     ]
    }
   ],
   "source": [
    "embedding_C, context_W = train(center_list, context_list, 1000, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# obj0, obj1, obj2 are created here...\n",
    "tmp_w = []\n",
    "tmp_c = []\n",
    "def save(idx = 0):\n",
    "    # Saving the objects:\n",
    "    global tmp_w, tmp_c\n",
    "    with open(f'skipgram-objs-{idx}.pkl', 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "        pickle.dump([tmp_w, tmp_c], f)\n",
    "\n",
    "def load(idx = 0):\n",
    "    # Getting back the objects:\n",
    "    global tmp_w, tmp_c\n",
    "    save(999)\n",
    "    with open(f'skipgram-objs-{idx}.pkl', 'rb') as f:  # Python 3: open(..., 'rb')\n",
    "        tmp_w, tmp_c = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "load(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_diff(word_1, word_2):\n",
    "    global tmp_c\n",
    "    vector_1 = tmp_c[word_id[word_1]]\n",
    "    vector_2 = tmp_c[word_id[word_2]]\n",
    "    print(fun.cosine_similarity(vector_1, vector_2, dim = 0))\n",
    "\n",
    "\n",
    "\n",
    "def show_vector(word_1):\n",
    "    global tmp_c\n",
    "    vector_1 = tmp_c[word_id[word_1]].detach().numpy()\n",
    "    print(vector_1)\n",
    "\n",
    "\n",
    "def predict(kata, K = 3):\n",
    "    global tmp_c\n",
    "    global tmp_w\n",
    "    print()\n",
    "    print(f\"Predicting kata setelah \\\"{kata}\\\"\")\n",
    "    C = tmp_c\n",
    "    W = tmp_w\n",
    "    X = get_input_tensor(torch.tensor([word_id[kata]])).float()\n",
    "    h = X.mm(C)\n",
    "    Y_pred = softmax(h.mm(W))[0]\n",
    "    prediction = sorted([(Y_pred[i], i) for i in range(len(Y_pred))], reverse=True)\n",
    "    # print(prediction)\n",
    "    for i in range(K):\n",
    "        print(id_word[prediction[i][1]])\n",
    "    # print(prediction)\n",
    "    # print(Y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.2722139   0.18811591  0.39483726  0.1276671   0.38488406  0.25605953\n",
      "  0.19266635  0.71376663 -0.7646854  -0.3206997 ]\n",
      "[-0.46989498 -0.12896392  0.4453655   0.14686778  0.50263804  0.1988348\n",
      "  0.02273537  0.7050972  -0.6474231  -0.41661516]\n",
      "[-0.03588893  0.11914188 -0.25533378  0.13690066  0.6920997   0.5876573\n",
      " -0.07938295  0.41481638  0.24626774  0.62607116]\n",
      "[-0.30892617  0.2908435   0.6487435   0.08201316  0.34246147  0.01942619\n",
      "  0.3103443   0.7628867  -0.6283434  -0.15487556]\n",
      "tensor(0.9444, grad_fn=<SumBackward1>)\n",
      "tensor(0.8743, grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "show_vector(\"gua\")\n",
    "show_vector(\"gue\")\n",
    "show_vector(\"aku\")\n",
    "show_vector(\"gw\")\n",
    "get_diff(\"gua\", \"gw\")\n",
    "get_diff(\"gue\", \"gw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4105, grad_fn=<SumBackward1>)\n",
      "tensor(0.6363, grad_fn=<SumBackward1>)\n",
      "tensor(0.3320, grad_fn=<SumBackward1>)\n",
      "tensor(0.8303, grad_fn=<SumBackward1>)\n",
      "tensor(0.9444, grad_fn=<SumBackward1>)\n",
      "tensor(0.8743, grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "get_diff(\"ngakak\", \"wkwk\")\n",
    "get_diff(\"lu\", \"bang\")\n",
    "get_diff(\"mantap\", \"keren\")\n",
    "get_diff(\"gua\", \"lu\") # ??\n",
    "get_diff(\"gua\", \"gw\")\n",
    "get_diff(\"gue\", \"gw\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting kata setelah \"tapi\"\n",
      "boong\n",
      "lu\n",
      "gua\n",
      "\n",
      "Predicting kata setelah \"hiya\"\n",
      "hiya\n",
      "ah\n",
      "a\n",
      "\n",
      "Predicting kata setelah \"bang\"\n",
      "lu\n",
      "gw\n",
      "lo\n",
      "\n",
      "Predicting kata setelah \"haters\"\n",
      "lu\n",
      "haters\n",
      "gua\n",
      "\n",
      "Predicting kata setelah \"gw\"\n",
      "suka\n",
      "bang\n",
      "lu\n"
     ]
    }
   ],
   "source": [
    "predict(\"tapi\")\n",
    "predict(\"hiya\")\n",
    "predict(\"bang\")\n",
    "predict(\"haters\")\n",
    "predict(\"gw\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('3.9.7')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "467d8379f95ad7eb63f1dc50c7908a840ac7d6ef9b8b78dbe6ade2a8663db7de"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
