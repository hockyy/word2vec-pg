{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as fun\n",
    "import numpy as np\n",
    "\n",
    "from google_play_scraper import Sort, reviews\n",
    "import csv\n",
    "\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "def clean_word(sentence):\n",
    "    sentence = wordpunct_tokenize(sentence)\n",
    "    cleaned = []\n",
    "    for word in sentence:\n",
    "        if(word.isalpha()):\n",
    "            cleaned.append(word.lower())\n",
    "    return cleaned\n",
    "\n",
    "def get_corpus_from_csv():\n",
    "    corpus = []\n",
    "    with open(\"dfyutup-cleanedfixversion3.csv\") as file_buffer:\n",
    "        csv_reader = list(csv.reader(file_buffer))\n",
    "        n = len(csv_reader)\n",
    "        for i in range(1, n):\n",
    "            corpus.append(clean_word(csv_reader[i][2]))\n",
    "    return corpus\n",
    "\n",
    "    \n",
    "def get_corpus_google_play(cnt = 100):\n",
    "    result, _ = reviews(\n",
    "        'com.apps.MyXL',\n",
    "        lang='id', \n",
    "        country='id', \n",
    "        sort=Sort.MOST_RELEVANT, \n",
    "        count=cnt, \n",
    "        filter_score_with=None\n",
    "    )\n",
    "    return [clean_word(konten[\"content\"]) for konten in result]\n",
    "\n",
    "# print(get_corpus_google_play())\n",
    "# print(get_corpus_from_csv())\n",
    "\n",
    "corpus = get_corpus_from_csv()\n",
    "# print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rick\n",
      "lu\n",
      "anak\n",
      "ya\n",
      "itu\n",
      "bang\n",
      "gw\n",
      "sub\n",
      "kontol\n",
      "apa\n",
      "di\n",
      "mu\n",
      "muka\n",
      "lo\n",
      "anjing\n",
      "gue\n",
      "nonton\n",
      "youtube\n",
      "suka\n",
      "gaya\n",
      "haha\n",
      "wkwkwkw\n",
      "buat\n",
      "haters\n",
      "semua\n",
      "sok\n",
      "video\n",
      "erickolim\n",
      "kalo\n",
      "gk\n",
      "ngapain\n",
      "ditonton\n",
      "eriko\n",
      "juga\n",
      "dia\n",
      "klo\n",
      "gak\n",
      "cuma\n",
      "doang\n",
      "kayak\n",
      "memek\n",
      "asu\n",
      "babi\n",
      "kecil\n",
      "v\n",
      "tapi\n",
      "ngakak\n",
      "ericko\n",
      "lim\n",
      "walaupun\n",
      "toxic\n",
      "mantap\n",
      "dah\n",
      "rik\n",
      "tuh\n",
      "aja\n",
      "gua\n",
      "konten\n",
      "adsense\n",
      "masih\n",
      "iya\n",
      "banget\n",
      "anjeng\n",
      "bikin\n",
      "vidio\n",
      "ada\n",
      "yg\n",
      "kali\n",
      "dari\n",
      "goblok\n",
      "ni\n",
      "bgt\n",
      "kek\n",
      "org\n",
      "subscribe\n",
      "trus\n",
      "komentar\n",
      "lagi\n",
      "biar\n",
      "hater\n",
      "kagak\n",
      "terus\n",
      "harus\n",
      "semangat\n",
      "aku\n",
      "tau\n",
      "terkenal\n",
      "bukan\n",
      "hal\n",
      "yang\n",
      "dan\n",
      "tetap\n",
      "eric\n",
      "jadi\n",
      "youtuber\n",
      "lebih\n",
      "sukses\n",
      "bisa\n",
      "erick\n",
      "fans\n",
      "karna\n",
      "kalau\n",
      "jangan\n",
      "bau\n",
      "karya\n",
      "sabar\n",
      "ngomong\n",
      "usah\n",
      "i\n",
      "love\n",
      "you\n",
      "auto\n",
      "subs\n",
      "like\n",
      "bilang\n",
      "sama\n",
      "orang\n",
      "jembut\n",
      "otak\n",
      "bg\n",
      "para\n",
      "tu\n",
      "banyak\n",
      "bro\n",
      "menghibur\n",
      "anti\n",
      "si\n",
      "mah\n",
      "k\n",
      "ajg\n",
      "wkwk\n",
      "nya\n",
      "tetep\n",
      "salah\n",
      "ganteng\n",
      "penting\n",
      "berkarya\n",
      "comment\n",
      "intro\n",
      "emang\n",
      "channel\n",
      "tanpa\n",
      "bakal\n",
      "ini\n",
      "hahaha\n",
      "heters\n",
      "tolol\n",
      "hiburan\n",
      "mendidik\n",
      "is\n",
      "the\n",
      "duit\n",
      "btw\n",
      "d\n",
      "jgn\n",
      "tpi\n",
      "kan\n",
      "bocah\n",
      "njing\n",
      "satu\n",
      "mati\n",
      "kapan\n",
      "komen\n",
      "bangsat\n",
      "eh\n",
      "subscriber\n",
      "atau\n",
      "lain\n",
      "sangat\n",
      "karena\n",
      "bego\n",
      "punya\n",
      "mending\n",
      "kyk\n",
      "gini\n",
      "kasar\n",
      "sumpah\n",
      "gila\n",
      "pada\n",
      "koment\n",
      "lah\n",
      "erickontol\n",
      "baru\n",
      "kocak\n",
      "pertama\n",
      "belum\n",
      "manis\n",
      "ah\n",
      "elu\n",
      "tai\n",
      "juta\n",
      "pake\n",
      "a\n",
      "good\n",
      "gimana\n",
      "coli\n",
      "pasti\n",
      "makin\n",
      "kaya\n",
      "amat\n",
      "ama\n",
      "setiap\n",
      "ngehate\n",
      "main\n",
      "udah\n",
      "tp\n",
      "mereka\n",
      "bener\n",
      "sih\n",
      "y\n",
      "liat\n",
      "g\n",
      "wkwkwk\n",
      "coment\n",
      "pas\n",
      "paling\n",
      "hidup\n",
      "hanya\n",
      "semoga\n",
      "kalian\n",
      "dapet\n",
      "wkwkwkwk\n",
      "untuk\n",
      "mana\n",
      "gitu\n",
      "boong\n",
      "bagi\n",
      "diri\n",
      "sendiri\n",
      "bacot\n",
      "ngentot\n",
      "x\n",
      "kok\n",
      "ngilu\n",
      "setuju\n",
      "and\n",
      "iri\n",
      "ric\n",
      "minta\n",
      "mirip\n",
      "dengan\n",
      "real\n",
      "coba\n",
      "lucu\n",
      "yt\n",
      "malah\n",
      "ketawa\n",
      "tonton\n",
      "dong\n",
      "erik\n",
      "so\n",
      "keren\n",
      "jdi\n",
      "jelek\n",
      "uang\n",
      "dasar\n",
      "ga\n",
      "anjir\n",
      "berfaedah\n",
      "makan\n",
      "berarti\n",
      "kenapa\n",
      "saya\n",
      "kak\n",
      "masuk\n",
      "dukung\n",
      "cuman\n",
      "kntl\n",
      "woy\n",
      "u\n",
      "pernah\n",
      "wkwkw\n",
      "arap\n",
      "soapers\n",
      "menit\n",
      "ke\n",
      "cara\n",
      "beda\n",
      "mungkin\n",
      "lg\n",
      "baik\n",
      "bagus\n",
      "mau\n",
      "bully\n",
      "seneng\n",
      "chanel\n",
      "anjg\n",
      "w\n",
      "gara\n",
      "ngentod\n",
      "erikontol\n",
      "lama\n",
      "hatters\n",
      "luh\n",
      "loh\n",
      "l\n",
      "tiap\n",
      "hari\n",
      "sampah\n",
      "salam\n",
      "sekarang\n",
      "hate\n",
      "kamu\n",
      "yaa\n",
      "pendidikan\n",
      "langsung\n",
      "videonya\n",
      "menurut\n",
      "soapking\n",
      "best\n",
      "t\n",
      "indonesia\n",
      "aj\n",
      "gausah\n",
      "fuck\n",
      "temen\n",
      "sampe\n",
      "upload\n",
      "erico\n",
      "no\n",
      "limit\n",
      "woi\n",
      "kita\n",
      "tidak\n",
      "beli\n",
      "skip\n",
      "anjink\n",
      "cari\n",
      "udh\n",
      "kaga\n",
      "saran\n",
      "in\n",
      "njir\n",
      "kata\n",
      "sekali\n",
      "tambah\n",
      "baca\n",
      "kurang\n",
      "jujur\n",
      "mikir\n",
      "rewind\n",
      "benci\n",
      "abang\n",
      "nge\n",
      "reza\n",
      "akan\n",
      "manusia\n",
      "kasih\n",
      "makanya\n",
      "nggak\n",
      "terhibur\n",
      "ea\n",
      "padahal\n",
      "sering\n",
      "ambil\n",
      "emng\n",
      "agama\n",
      "friendly\n",
      "kebanyakan\n",
      "bgst\n",
      "tolong\n",
      "ngatain\n",
      "dunia\n",
      "hati\n",
      "jt\n",
      "yah\n",
      "m\n",
      "jahat\n",
      "namanya\n",
      "selalu\n",
      "nih\n",
      "hapus\n",
      "anda\n",
      "adalah\n",
      "oh\n",
      "kau\n",
      "tod\n",
      "king\n",
      "loe\n",
      "jelas\n",
      "lagu\n",
      "boleh\n",
      "sini\n",
      "family\n",
      "wik\n",
      "support\n",
      "siapa\n",
      "gblk\n",
      "heaters\n",
      "hiya\n",
      "dulu\n",
      "dislike\n",
      "opening\n",
      "akhirnya\n",
      "botak\n",
      "pengen\n",
      "dengerin\n",
      "an\n",
      "dalam\n",
      "ngga\n",
      "nama\n",
      "biasa\n",
      "qorygore\n",
      "iklan\n",
      "back\n",
      "ko\n",
      "iklannya\n",
      "tung\n",
      "legend\n",
      "{'rick': 0, 'lu': 1, 'anak': 2, 'ya': 3, 'itu': 4, 'bang': 5, 'gw': 6, 'sub': 7, 'kontol': 8, 'apa': 9, 'di': 10, 'mu': 11, 'muka': 12, 'lo': 13, 'anjing': 14, 'gue': 15, 'nonton': 16, 'youtube': 17, 'suka': 18, 'gaya': 19, 'haha': 20, 'wkwkwkw': 21, 'buat': 22, 'haters': 23, 'semua': 24, 'sok': 25, 'video': 26, 'erickolim': 27, 'kalo': 28, 'gk': 29, 'ngapain': 30, 'ditonton': 31, 'eriko': 32, 'juga': 33, 'dia': 34, 'klo': 35, 'gak': 36, 'cuma': 37, 'doang': 38, 'kayak': 39, 'memek': 40, 'asu': 41, 'babi': 42, 'kecil': 43, 'v': 44, 'tapi': 45, 'ngakak': 46, 'ericko': 47, 'lim': 48, 'walaupun': 49, 'toxic': 50, 'mantap': 51, 'dah': 52, 'rik': 53, 'tuh': 54, 'aja': 55, 'gua': 56, 'konten': 57, 'adsense': 58, 'masih': 59, 'iya': 60, 'banget': 61, 'anjeng': 62, 'bikin': 63, 'vidio': 64, 'ada': 65, 'yg': 66, 'kali': 67, 'dari': 68, 'goblok': 69, 'ni': 70, 'bgt': 71, 'kek': 72, 'org': 73, 'subscribe': 74, 'trus': 75, 'komentar': 76, 'lagi': 77, 'biar': 78, 'hater': 79, 'kagak': 80, 'terus': 81, 'harus': 82, 'semangat': 83, 'aku': 84, 'tau': 85, 'terkenal': 86, 'bukan': 87, 'hal': 88, 'yang': 89, 'dan': 90, 'tetap': 91, 'eric': 92, 'jadi': 93, 'youtuber': 94, 'lebih': 95, 'sukses': 96, 'bisa': 97, 'erick': 98, 'fans': 99, 'karna': 100, 'kalau': 101, 'jangan': 102, 'bau': 103, 'karya': 104, 'sabar': 105, 'ngomong': 106, 'usah': 107, 'i': 108, 'love': 109, 'you': 110, 'auto': 111, 'subs': 112, 'like': 113, 'bilang': 114, 'sama': 115, 'orang': 116, 'jembut': 117, 'otak': 118, 'bg': 119, 'para': 120, 'tu': 121, 'banyak': 122, 'bro': 123, 'menghibur': 124, 'anti': 125, 'si': 126, 'mah': 127, 'k': 128, 'ajg': 129, 'wkwk': 130, 'nya': 131, 'tetep': 132, 'salah': 133, 'ganteng': 134, 'penting': 135, 'berkarya': 136, 'comment': 137, 'intro': 138, 'emang': 139, 'channel': 140, 'tanpa': 141, 'bakal': 142, 'ini': 143, 'hahaha': 144, 'heters': 145, 'tolol': 146, 'hiburan': 147, 'mendidik': 148, 'is': 149, 'the': 150, 'duit': 151, 'btw': 152, 'd': 153, 'jgn': 154, 'tpi': 155, 'kan': 156, 'bocah': 157, 'njing': 158, 'satu': 159, 'mati': 160, 'kapan': 161, 'komen': 162, 'bangsat': 163, 'eh': 164, 'subscriber': 165, 'atau': 166, 'lain': 167, 'sangat': 168, 'karena': 169, 'bego': 170, 'punya': 171, 'mending': 172, 'kyk': 173, 'gini': 174, 'kasar': 175, 'sumpah': 176, 'gila': 177, 'pada': 178, 'koment': 179, 'lah': 180, 'erickontol': 181, 'baru': 182, 'kocak': 183, 'pertama': 184, 'belum': 185, 'manis': 186, 'ah': 187, 'elu': 188, 'tai': 189, 'juta': 190, 'pake': 191, 'a': 192, 'good': 193, 'gimana': 194, 'coli': 195, 'pasti': 196, 'makin': 197, 'kaya': 198, 'amat': 199, 'ama': 200, 'setiap': 201, 'ngehate': 202, 'main': 203, 'udah': 204, 'tp': 205, 'mereka': 206, 'bener': 207, 'sih': 208, 'y': 209, 'liat': 210, 'g': 211, 'wkwkwk': 212, 'coment': 213, 'pas': 214, 'paling': 215, 'hidup': 216, 'hanya': 217, 'semoga': 218, 'kalian': 219, 'dapet': 220, 'wkwkwkwk': 221, 'untuk': 222, 'mana': 223, 'gitu': 224, 'boong': 225, 'bagi': 226, 'diri': 227, 'sendiri': 228, 'bacot': 229, 'ngentot': 230, 'x': 231, 'kok': 232, 'ngilu': 233, 'setuju': 234, 'and': 235, 'iri': 236, 'ric': 237, 'minta': 238, 'mirip': 239, 'dengan': 240, 'real': 241, 'coba': 242, 'lucu': 243, 'yt': 244, 'malah': 245, 'ketawa': 246, 'tonton': 247, 'dong': 248, 'erik': 249, 'so': 250, 'keren': 251, 'jdi': 252, 'jelek': 253, 'uang': 254, 'dasar': 255, 'ga': 256, 'anjir': 257, 'berfaedah': 258, 'makan': 259, 'berarti': 260, 'kenapa': 261, 'saya': 262, 'kak': 263, 'masuk': 264, 'dukung': 265, 'cuman': 266, 'kntl': 267, 'woy': 268, 'u': 269, 'pernah': 270, 'wkwkw': 271, 'arap': 272, 'soapers': 273, 'menit': 274, 'ke': 275, 'cara': 276, 'beda': 277, 'mungkin': 278, 'lg': 279, 'baik': 280, 'bagus': 281, 'mau': 282, 'bully': 283, 'seneng': 284, 'chanel': 285, 'anjg': 286, 'w': 287, 'gara': 288, 'ngentod': 289, 'erikontol': 290, 'lama': 291, 'hatters': 292, 'luh': 293, 'loh': 294, 'l': 295, 'tiap': 296, 'hari': 297, 'sampah': 298, 'salam': 299, 'sekarang': 300, 'hate': 301, 'kamu': 302, 'yaa': 303, 'pendidikan': 304, 'langsung': 305, 'videonya': 306, 'menurut': 307, 'soapking': 308, 'best': 309, 't': 310, 'indonesia': 311, 'aj': 312, 'gausah': 313, 'fuck': 314, 'temen': 315, 'sampe': 316, 'upload': 317, 'erico': 318, 'no': 319, 'limit': 320, 'woi': 321, 'kita': 322, 'tidak': 323, 'beli': 324, 'skip': 325, 'anjink': 326, 'cari': 327, 'udh': 328, 'kaga': 329, 'saran': 330, 'in': 331, 'njir': 332, 'kata': 333, 'sekali': 334, 'tambah': 335, 'baca': 336, 'kurang': 337, 'jujur': 338, 'mikir': 339, 'rewind': 340, 'benci': 341, 'abang': 342, 'nge': 343, 'reza': 344, 'akan': 345, 'manusia': 346, 'kasih': 347, 'makanya': 348, 'nggak': 349, 'terhibur': 350, 'ea': 351, 'padahal': 352, 'sering': 353, 'ambil': 354, 'emng': 355, 'agama': 356, 'friendly': 357, 'kebanyakan': 358, 'bgst': 359, 'tolong': 360, 'ngatain': 361, 'dunia': 362, 'hati': 363, 'jt': 364, 'yah': 365, 'm': 366, 'jahat': 367, 'namanya': 368, 'selalu': 369, 'nih': 370, 'hapus': 371, 'anda': 372, 'adalah': 373, 'oh': 374, 'kau': 375, 'tod': 376, 'king': 377, 'loe': 378, 'jelas': 379, 'lagu': 380, 'boleh': 381, 'sini': 382, 'family': 383, 'wik': 384, 'support': 385, 'siapa': 386, 'gblk': 387, 'heaters': 388, 'hiya': 389, 'dulu': 390, 'dislike': 391, 'opening': 392, 'akhirnya': 393, 'botak': 394, 'pengen': 395, 'dengerin': 396, 'an': 397, 'dalam': 398, 'ngga': 399, 'nama': 400, 'biasa': 401, 'qorygore': 402, 'iklan': 403, 'back': 404, 'ko': 405, 'iklannya': 406, 'tung': 407, 'legend': 408}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as fun\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def create_vocab(corpus, limit_occ = 50):\n",
    "    \"\"\"\n",
    "    membuat vocabulary, dengan 2 info penting: \n",
    "        1. word_id -> mapping kata ke id (int)\n",
    "        2. id_word -> sebaliknya, mapping dari id ke word\n",
    "    \"\"\"\n",
    "    word_id = {}\n",
    "    id_word = []\n",
    "\n",
    "    occurence = dict()\n",
    "    i = 0\n",
    "    for sentence in corpus:\n",
    "        for word in sentence:\n",
    "            occurence.setdefault(word, 0)\n",
    "            occurence[word] += 1\n",
    "            \n",
    "    for k, v in occurence.items():\n",
    "        if(v < limit_occ): continue\n",
    "        print(k)\n",
    "        word_id[k] = i\n",
    "        id_word.append(k)\n",
    "        i += 1\n",
    "                \n",
    "    id_word = {id: word for (word, id) in word_id.items()}\n",
    "    return word_id, id_word\n",
    "\n",
    "\n",
    "word_id, id_word = create_vocab(corpus)\n",
    "vocab_size = len(word_id)\n",
    "\n",
    "print(word_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset generator\n",
    "def iter_dataset(corpus, context_window=1):\n",
    "    for sentence in corpus:\n",
    "        words = sentence\n",
    "        for i in range(len(words)):\n",
    "            center_word = words[i]\n",
    "            # cari konteks ke kiri\n",
    "            j = i - 1\n",
    "            while (j >= i - context_window) and (j >= 0):\n",
    "                context_word = words[j]\n",
    "                yield (center_word, context_word)\n",
    "                j -= 1\n",
    "            # cari konteks ke kanan\n",
    "            j = i + 1\n",
    "            while (j <= i + context_window) and (j < len(words)):\n",
    "                context_word = words[j]\n",
    "                yield (center_word, context_word)\n",
    "                j += 1\n",
    "\n",
    "\n",
    "def get_dataset(corpus):\n",
    "    # membuat dataset\n",
    "    X = []\n",
    "    Y = []\n",
    "    for center, context in iter_dataset(corpus):\n",
    "        try:\n",
    "            if(center not in word_id.keys() or context not in word_id.keys()):\n",
    "                continue\n",
    "            X.append(word_id[center])\n",
    "            Y.append(word_id[context])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    X_ctr = torch.tensor(X)\n",
    "    Y_ctx = torch.tensor(Y)\n",
    "    return X_ctr, Y_ctx\n",
    "\n",
    "\n",
    "def get_input_tensor(tensor):\n",
    "    return fun.one_hot(tensor, num_classes=vocab_size)\n",
    "\n",
    "\n",
    "center_list, context_list = get_dataset(corpus)\n",
    "# get_input_tensor(center_list[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax\n",
    "def softmax(x):\n",
    "    maxes = torch.max(x, 1, keepdim=True)[0]\n",
    "    x_exp = torch.exp(x - maxes)\n",
    "    x_exp_sum = torch.sum(x_exp, 1, keepdim=True)\n",
    "    return x_exp / x_exp_sum\n",
    "\n",
    "# Categorical Cross Entropy Loss\n",
    "\n",
    "\n",
    "def CCEloss(Y_pred, Y_true):\n",
    "    m = Y_pred.size()[0]\n",
    "    return -(1 / m) * torch.sum(Y_true * torch.log(Y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_c = None\n",
    "tmp_w = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C shape is: torch.Size([409, 5]), W shape is: torch.Size([5, 409])\n",
      "tensor([[-0.0995,  0.0102, -0.0447, -0.0807, -0.0814],\n",
      "        [ 0.0610,  0.0906, -0.0281, -0.0176, -0.0089],\n",
      "        [ 0.0777,  0.0550, -0.0556, -0.0927, -0.0088],\n",
      "        ...,\n",
      "        [ 0.0422, -0.0273,  0.0035, -0.0918, -0.0574],\n",
      "        [-0.0695, -0.0651, -0.0851, -0.0298, -0.0688],\n",
      "        [ 0.0846, -0.0459,  0.0214,  0.0025, -0.0558]], requires_grad=True)\n",
      "tensor([[-0.0534, -0.0683, -0.0326,  ...,  0.0691, -0.0342, -0.0645],\n",
      "        [-0.0272,  0.0379,  0.0603,  ..., -0.0853,  0.0342,  0.0274],\n",
      "        [-0.0303,  0.0429,  0.0806,  ...,  0.0990, -0.0245,  0.0684],\n",
      "        [-0.0182, -0.0391, -0.0305,  ...,  0.0790,  0.0973, -0.0115],\n",
      "        [-0.0230, -0.0438,  0.0288,  ..., -0.0514,  0.0224, -0.0278]],\n",
      "       requires_grad=True)\n",
      "Epoch 0, loss = 6.013712406158447, sim(lo, lu) = 0.45380493998527527\n",
      "Epoch 10, loss = 6.0135297775268555, sim(lo, lu) = 0.46046119928359985\n",
      "Epoch 20, loss = 6.013360023498535, sim(lo, lu) = 0.4671841561794281\n",
      "Epoch 30, loss = 6.013179302215576, sim(lo, lu) = 0.4739704728126526\n",
      "Epoch 40, loss = 6.012983322143555, sim(lo, lu) = 0.4808163344860077\n",
      "Epoch 50, loss = 6.012763977050781, sim(lo, lu) = 0.4877174496650696\n",
      "Epoch 60, loss = 6.01251220703125, sim(lo, lu) = 0.49467015266418457\n",
      "Epoch 70, loss = 6.012219429016113, sim(lo, lu) = 0.5016703009605408\n",
      "Epoch 80, loss = 6.011876583099365, sim(lo, lu) = 0.50871342420578\n",
      "Epoch 90, loss = 6.011467933654785, sim(lo, lu) = 0.5157952904701233\n",
      "Epoch 100, loss = 6.010978698730469, sim(lo, lu) = 0.5229111909866333\n",
      "Epoch 110, loss = 6.010389804840088, sim(lo, lu) = 0.5300567150115967\n",
      "Epoch 120, loss = 6.009678840637207, sim(lo, lu) = 0.5372269153594971\n",
      "Epoch 130, loss = 6.008815288543701, sim(lo, lu) = 0.5444172620773315\n",
      "Epoch 140, loss = 6.007765769958496, sim(lo, lu) = 0.5516225695610046\n",
      "Epoch 150, loss = 6.006488800048828, sim(lo, lu) = 0.5588377714157104\n",
      "Epoch 160, loss = 6.004931449890137, sim(lo, lu) = 0.5660576820373535\n",
      "Epoch 170, loss = 6.003028392791748, sim(lo, lu) = 0.5732771158218384\n",
      "Epoch 180, loss = 6.00070333480835, sim(lo, lu) = 0.5804906487464905\n",
      "Epoch 190, loss = 5.997860908508301, sim(lo, lu) = 0.5876929759979248\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def train(X_ctr, Y_ctx, EPOCHS = 200, load = False):\n",
    "    global tmp_w\n",
    "    global tmp_c\n",
    "\n",
    "    EMBEDDING_DIMS = 5\n",
    "\n",
    "    initrange = 0.5 / EMBEDDING_DIMS\n",
    "\n",
    "    # trick: di awal, mode gradient jangan diaktifkan dahulu, karena ingin\n",
    "    # memodifikasi nilai di C dan W secara in-place\n",
    "    C = torch.rand(vocab_size, EMBEDDING_DIMS, requires_grad = False)\n",
    "    W = torch.rand(EMBEDDING_DIMS, vocab_size, requires_grad = False)\n",
    "\n",
    "    # agar nilai awal parameter di C dan W berkisar di antara -initrange hingga +initrange\n",
    "    C = -2 * initrange * C + initrange\n",
    "    W = -2 * initrange * W + initrange\n",
    "\n",
    "    # setelah operasi in-place modification, baru kita set mode gradient\n",
    "    C.requires_grad = True\n",
    "    W.requires_grad = True\n",
    "    if(load):\n",
    "        C = tmp_c\n",
    "        W = tmp_w\n",
    "\n",
    "    print(f'C shape is: {C.shape}, W shape is: {W.shape}')\n",
    "    print(C)\n",
    "    print(W)\n",
    "\n",
    "    LEARNING_RATE = 0.2\n",
    "    LR_DECAY = 0.99\n",
    "\n",
    "    for i in range(EPOCHS):\n",
    "        X = get_input_tensor(X_ctr).float()\n",
    "        Y = get_input_tensor(Y_ctx).float()\n",
    "\n",
    "        h = X.mm(C)\n",
    "        Y_pred = softmax(h.mm(W))\n",
    "\n",
    "        loss = CCEloss(Y_pred, Y)\n",
    "        loss.backward()\n",
    "\n",
    "        # update C dan W\n",
    "        with torch.no_grad():\n",
    "            C -= LEARNING_RATE * C.grad\n",
    "            W -= LEARNING_RATE * W.grad\n",
    "            \n",
    "            C.grad.zero_()\n",
    "            W.grad.zero_()\n",
    "        tmp_w = W\n",
    "        tmp_c = C\n",
    "        if i % 10 == 0:\n",
    "            # kita coba lihat progress cosine similarity\n",
    "            # antara embedding \"lo\" dengan \"lu\"\n",
    "            vector_1 = C[word_id[\"lo\"]]\n",
    "            vector_2 = C[word_id[\"lu\"]]\n",
    "            sim = fun.cosine_similarity(vector_1, vector_2, dim = 0)\n",
    "            print(f'Epoch {i}, loss = {loss}, sim(lo, lu) = {sim}')\n",
    "    return C, W\n",
    "# embedding_C, context_W = train(center_list, context_list, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C shape is: torch.Size([409, 5]), W shape is: torch.Size([5, 409])\n",
      "tensor([[-0.1011,  0.0102, -0.0417, -0.0814, -0.0808],\n",
      "        [ 0.0655,  0.0832, -0.0152, -0.0335,  0.0027],\n",
      "        [ 0.0767,  0.0557, -0.0519, -0.0920, -0.0075],\n",
      "        ...,\n",
      "        [ 0.0424, -0.0278,  0.0042, -0.0919, -0.0576],\n",
      "        [-0.0770, -0.0594, -0.0912, -0.0120, -0.0661],\n",
      "        [ 0.0838, -0.0447,  0.0229,  0.0014, -0.0541]], requires_grad=True)\n",
      "tensor([[-0.0520, -0.0553, -0.0318,  ...,  0.0688, -0.0478, -0.0658],\n",
      "        [-0.0253,  0.0357,  0.0635,  ..., -0.0839,  0.0244,  0.0293],\n",
      "        [-0.0278,  0.0495,  0.0792,  ...,  0.0991, -0.0404,  0.0693],\n",
      "        [-0.0199, -0.0578, -0.0322,  ...,  0.0801,  0.0944, -0.0116],\n",
      "        [-0.0225, -0.0444,  0.0291,  ..., -0.0516,  0.0102, -0.0304]],\n",
      "       requires_grad=True)\n",
      "Epoch 0, loss = 5.994383811950684, sim(lo, lu) = 0.5948783159255981\n",
      "Epoch 10, loss = 5.990130424499512, sim(lo, lu) = 0.6020410060882568\n",
      "Epoch 20, loss = 5.984925270080566, sim(lo, lu) = 0.6091755628585815\n",
      "Epoch 30, loss = 5.978556156158447, sim(lo, lu) = 0.6162759065628052\n",
      "Epoch 40, loss = 5.970767498016357, sim(lo, lu) = 0.6233361959457397\n",
      "Epoch 50, loss = 5.961244583129883, sim(lo, lu) = 0.6303505301475525\n",
      "Epoch 60, loss = 5.949611663818359, sim(lo, lu) = 0.6373128890991211\n",
      "Epoch 70, loss = 5.935428619384766, sim(lo, lu) = 0.6442171335220337\n",
      "Epoch 80, loss = 5.918189525604248, sim(lo, lu) = 0.6510569453239441\n",
      "Epoch 90, loss = 5.897355079650879, sim(lo, lu) = 0.6578263640403748\n",
      "Epoch 100, loss = 5.872463703155518, sim(lo, lu) = 0.6645190119743347\n",
      "Epoch 110, loss = 5.843427658081055, sim(lo, lu) = 0.6711291074752808\n",
      "Epoch 120, loss = 5.811272144317627, sim(lo, lu) = 0.6776511669158936\n",
      "Epoch 130, loss = 5.779303073883057, sim(lo, lu) = 0.6840811371803284\n",
      "Epoch 140, loss = 5.752911567687988, sim(lo, lu) = 0.6904178857803345\n",
      "Epoch 150, loss = 5.7353620529174805, sim(lo, lu) = 0.696662962436676\n",
      "Epoch 160, loss = 5.725186824798584, sim(lo, lu) = 0.7028199434280396\n",
      "Epoch 170, loss = 5.719411849975586, sim(lo, lu) = 0.708891749382019\n",
      "Epoch 180, loss = 5.715989112854004, sim(lo, lu) = 0.7148801684379578\n",
      "Epoch 190, loss = 5.713834762573242, sim(lo, lu) = 0.7207858562469482\n",
      "Epoch 200, loss = 5.712396144866943, sim(lo, lu) = 0.7266088724136353\n",
      "Epoch 210, loss = 5.711385726928711, sim(lo, lu) = 0.7323487997055054\n",
      "Epoch 220, loss = 5.710644245147705, sim(lo, lu) = 0.7380050420761108\n",
      "Epoch 230, loss = 5.710078239440918, sim(lo, lu) = 0.7435770034790039\n",
      "Epoch 240, loss = 5.709630966186523, sim(lo, lu) = 0.7490637302398682\n",
      "Epoch 250, loss = 5.7092671394348145, sim(lo, lu) = 0.7544647455215454\n",
      "Epoch 260, loss = 5.708962917327881, sim(lo, lu) = 0.759779155254364\n",
      "Epoch 270, loss = 5.708701133728027, sim(lo, lu) = 0.765006422996521\n",
      "Epoch 280, loss = 5.708471775054932, sim(lo, lu) = 0.7701460123062134\n",
      "Epoch 290, loss = 5.708266735076904, sim(lo, lu) = 0.7751973867416382\n",
      "Epoch 300, loss = 5.708078384399414, sim(lo, lu) = 0.7801600694656372\n",
      "Epoch 310, loss = 5.707903861999512, sim(lo, lu) = 0.7850339412689209\n",
      "Epoch 320, loss = 5.707738399505615, sim(lo, lu) = 0.7898187041282654\n",
      "Epoch 330, loss = 5.70758056640625, sim(lo, lu) = 0.7945143580436707\n",
      "Epoch 340, loss = 5.70742654800415, sim(lo, lu) = 0.7991207242012024\n",
      "Epoch 350, loss = 5.707277297973633, sim(lo, lu) = 0.8036379814147949\n",
      "Epoch 360, loss = 5.707128524780273, sim(lo, lu) = 0.8080663681030273\n",
      "Epoch 370, loss = 5.7069807052612305, sim(lo, lu) = 0.8124062418937683\n",
      "Epoch 380, loss = 5.706832408905029, sim(lo, lu) = 0.8166579008102417\n",
      "Epoch 390, loss = 5.706683158874512, sim(lo, lu) = 0.8208217620849609\n",
      "Epoch 400, loss = 5.7065324783325195, sim(lo, lu) = 0.8248984813690186\n",
      "Epoch 410, loss = 5.706379413604736, sim(lo, lu) = 0.8288886547088623\n",
      "Epoch 420, loss = 5.7062225341796875, sim(lo, lu) = 0.8327928185462952\n",
      "Epoch 430, loss = 5.706061840057373, sim(lo, lu) = 0.8366119861602783\n",
      "Epoch 440, loss = 5.705896854400635, sim(lo, lu) = 0.8403466939926147\n",
      "Epoch 450, loss = 5.705727577209473, sim(lo, lu) = 0.8439982533454895\n",
      "Epoch 460, loss = 5.7055535316467285, sim(lo, lu) = 0.8475672006607056\n",
      "Epoch 470, loss = 5.705373764038086, sim(lo, lu) = 0.8510547876358032\n",
      "Epoch 480, loss = 5.705188274383545, sim(lo, lu) = 0.8544621467590332\n",
      "Epoch 490, loss = 5.704996585845947, sim(lo, lu) = 0.8577901124954224\n"
     ]
    }
   ],
   "source": [
    "\n",
    "embedding_C, context_W = train(center_list, context_list, 500, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C shape is: torch.Size([409, 5]), W shape is: torch.Size([5, 409])\n",
      "tensor([[-0.1022,  0.0120, -0.0320, -0.0854, -0.0790],\n",
      "        [ 0.0872,  0.0722,  0.0290, -0.0843,  0.0346],\n",
      "        [ 0.0754,  0.0591, -0.0420, -0.0912, -0.0042],\n",
      "        ...,\n",
      "        [ 0.0430, -0.0286,  0.0058, -0.0921, -0.0581],\n",
      "        [-0.1071, -0.0502, -0.1193,  0.0332, -0.0685],\n",
      "        [ 0.0819, -0.0415,  0.0268, -0.0012, -0.0499]], requires_grad=True)\n",
      "tensor([[-0.0470, -0.0240, -0.0296,  ...,  0.0681, -0.0894, -0.0689],\n",
      "        [-0.0172,  0.0297,  0.0738,  ..., -0.0773,  0.0067,  0.0371],\n",
      "        [-0.0190,  0.0773,  0.0760,  ...,  0.0989, -0.0873,  0.0712],\n",
      "        [-0.0253, -0.1133, -0.0356,  ...,  0.0845,  0.1030, -0.0103],\n",
      "        [-0.0195, -0.0450,  0.0306,  ..., -0.0518, -0.0193, -0.0367]],\n",
      "       requires_grad=True)\n",
      "Epoch 0, loss = 5.704797744750977, sim(lo, lu) = 0.8610402345657349\n",
      "Epoch 10, loss = 5.704591751098633, sim(lo, lu) = 0.864213228225708\n",
      "Epoch 20, loss = 5.704378604888916, sim(lo, lu) = 0.867310643196106\n",
      "Epoch 30, loss = 5.70415735244751, sim(lo, lu) = 0.8703336715698242\n",
      "Epoch 40, loss = 5.703927993774414, sim(lo, lu) = 0.873283326625824\n",
      "Epoch 50, loss = 5.703690052032471, sim(lo, lu) = 0.8761613965034485\n",
      "Epoch 60, loss = 5.703442096710205, sim(lo, lu) = 0.8789687156677246\n",
      "Epoch 70, loss = 5.703185081481934, sim(lo, lu) = 0.8817068338394165\n",
      "Epoch 80, loss = 5.702917098999023, sim(lo, lu) = 0.884377121925354\n",
      "Epoch 90, loss = 5.702639579772949, sim(lo, lu) = 0.8869808912277222\n",
      "Epoch 100, loss = 5.702351093292236, sim(lo, lu) = 0.8895193934440613\n",
      "Epoch 110, loss = 5.702049732208252, sim(lo, lu) = 0.8919941186904907\n",
      "Epoch 120, loss = 5.701736927032471, sim(lo, lu) = 0.8944063186645508\n",
      "Epoch 130, loss = 5.701411724090576, sim(lo, lu) = 0.8967576622962952\n",
      "Epoch 140, loss = 5.701073169708252, sim(lo, lu) = 0.8990490436553955\n",
      "Epoch 150, loss = 5.70072078704834, sim(lo, lu) = 0.9012820720672607\n",
      "Epoch 160, loss = 5.700353622436523, sim(lo, lu) = 0.9034579396247864\n",
      "Epoch 170, loss = 5.6999711990356445, sim(lo, lu) = 0.9055783748626709\n",
      "Epoch 180, loss = 5.699572563171387, sim(lo, lu) = 0.9076442718505859\n",
      "Epoch 190, loss = 5.699158191680908, sim(lo, lu) = 0.9096570014953613\n",
      "Epoch 200, loss = 5.698726177215576, sim(lo, lu) = 0.9116181135177612\n",
      "Epoch 210, loss = 5.698276996612549, sim(lo, lu) = 0.913528323173523\n",
      "Epoch 220, loss = 5.697807788848877, sim(lo, lu) = 0.9153896570205688\n",
      "Epoch 230, loss = 5.697319507598877, sim(lo, lu) = 0.917202889919281\n",
      "Epoch 240, loss = 5.696810245513916, sim(lo, lu) = 0.9189692735671997\n",
      "Epoch 250, loss = 5.696280002593994, sim(lo, lu) = 0.9206901788711548\n",
      "Epoch 260, loss = 5.695727348327637, sim(lo, lu) = 0.9223666191101074\n",
      "Epoch 270, loss = 5.695151329040527, sim(lo, lu) = 0.9239999055862427\n",
      "Epoch 280, loss = 5.69455099105835, sim(lo, lu) = 0.925590991973877\n",
      "Epoch 290, loss = 5.693924903869629, sim(lo, lu) = 0.9271411895751953\n",
      "Epoch 300, loss = 5.693271636962891, sim(lo, lu) = 0.9286514520645142\n",
      "Epoch 310, loss = 5.692591667175293, sim(lo, lu) = 0.9301228523254395\n",
      "Epoch 320, loss = 5.691883087158203, sim(lo, lu) = 0.9315565824508667\n",
      "Epoch 330, loss = 5.691143989562988, sim(lo, lu) = 0.9329536557197571\n",
      "Epoch 340, loss = 5.690372467041016, sim(lo, lu) = 0.9343152046203613\n",
      "Epoch 350, loss = 5.689568996429443, sim(lo, lu) = 0.935641884803772\n",
      "Epoch 360, loss = 5.6887311935424805, sim(lo, lu) = 0.936934769153595\n",
      "Epoch 370, loss = 5.687857627868652, sim(lo, lu) = 0.938194990158081\n",
      "Epoch 380, loss = 5.68694543838501, sim(lo, lu) = 0.9394230842590332\n",
      "Epoch 390, loss = 5.685995101928711, sim(lo, lu) = 0.9406203031539917\n",
      "Epoch 400, loss = 5.685004711151123, sim(lo, lu) = 0.941787600517273\n",
      "Epoch 410, loss = 5.6839704513549805, sim(lo, lu) = 0.9429255127906799\n",
      "Epoch 420, loss = 5.682891845703125, sim(lo, lu) = 0.9440349340438843\n",
      "Epoch 430, loss = 5.681768417358398, sim(lo, lu) = 0.9451167583465576\n",
      "Epoch 440, loss = 5.680595397949219, sim(lo, lu) = 0.9461716413497925\n",
      "Epoch 450, loss = 5.679372787475586, sim(lo, lu) = 0.9472005367279053\n",
      "Epoch 460, loss = 5.678098201751709, sim(lo, lu) = 0.9482040405273438\n",
      "Epoch 470, loss = 5.6767683029174805, sim(lo, lu) = 0.9491829872131348\n",
      "Epoch 480, loss = 5.675381183624268, sim(lo, lu) = 0.9501379728317261\n",
      "Epoch 490, loss = 5.673933506011963, sim(lo, lu) = 0.951069712638855\n"
     ]
    }
   ],
   "source": [
    "embedding_C, context_W = train(center_list, context_list, 500, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C shape is: torch.Size([409, 5]), W shape is: torch.Size([5, 409])\n",
      "tensor([[-0.1000,  0.0135, -0.0183, -0.0951, -0.0765],\n",
      "        [ 0.1282,  0.0605,  0.0980, -0.1647,  0.0721],\n",
      "        [ 0.0752,  0.0631, -0.0308, -0.0927, -0.0008],\n",
      "        ...,\n",
      "        [ 0.0439, -0.0294,  0.0076, -0.0925, -0.0586],\n",
      "        [-0.1604, -0.0494, -0.1729,  0.0868, -0.0849],\n",
      "        [ 0.0800, -0.0384,  0.0309, -0.0039, -0.0459]], requires_grad=True)\n",
      "tensor([[-0.0437,  0.0188, -0.0311,  ...,  0.0625, -0.1540, -0.0769],\n",
      "        [-0.0139,  0.0247,  0.0802,  ..., -0.0755, -0.0132,  0.0402],\n",
      "        [-0.0088,  0.1311,  0.0708,  ...,  0.0942, -0.1564,  0.0692],\n",
      "        [-0.0371, -0.1912, -0.0406,  ...,  0.0888,  0.1334, -0.0093],\n",
      "        [-0.0144, -0.0443,  0.0327,  ..., -0.0519, -0.0530, -0.0433]],\n",
      "       requires_grad=True)\n",
      "Epoch 0, loss = 5.672426223754883, sim(lo, lu) = 0.9519789218902588\n",
      "Epoch 10, loss = 5.670852184295654, sim(lo, lu) = 0.9528663158416748\n",
      "Epoch 20, loss = 5.669210910797119, sim(lo, lu) = 0.9537322521209717\n",
      "Epoch 30, loss = 5.667500019073486, sim(lo, lu) = 0.954577624797821\n",
      "Epoch 40, loss = 5.66571569442749, sim(lo, lu) = 0.9554027318954468\n",
      "Epoch 50, loss = 5.663855075836182, sim(lo, lu) = 0.9562084078788757\n",
      "Epoch 60, loss = 5.661914825439453, sim(lo, lu) = 0.9569951295852661\n",
      "Epoch 70, loss = 5.659891128540039, sim(lo, lu) = 0.9577634930610657\n",
      "Epoch 80, loss = 5.657781600952148, sim(lo, lu) = 0.9585138559341431\n",
      "Epoch 90, loss = 5.655582904815674, sim(lo, lu) = 0.9592468738555908\n",
      "Epoch 100, loss = 5.653289794921875, sim(lo, lu) = 0.9599629044532776\n",
      "Epoch 110, loss = 5.650899410247803, sim(lo, lu) = 0.9606626629829407\n",
      "Epoch 120, loss = 5.648407936096191, sim(lo, lu) = 0.9613462090492249\n",
      "Epoch 130, loss = 5.645811080932617, sim(lo, lu) = 0.9620144367218018\n",
      "Epoch 140, loss = 5.643105506896973, sim(lo, lu) = 0.9626675844192505\n",
      "Epoch 150, loss = 5.640285968780518, sim(lo, lu) = 0.9633059501647949\n",
      "Epoch 160, loss = 5.637348175048828, sim(lo, lu) = 0.9639301300048828\n",
      "Epoch 170, loss = 5.634288311004639, sim(lo, lu) = 0.9645406007766724\n",
      "Epoch 180, loss = 5.631101131439209, sim(lo, lu) = 0.9651374816894531\n",
      "Epoch 190, loss = 5.627781391143799, sim(lo, lu) = 0.9657213687896729\n",
      "Epoch 200, loss = 5.624326705932617, sim(lo, lu) = 0.9662923216819763\n",
      "Epoch 210, loss = 5.620731830596924, sim(lo, lu) = 0.9668511152267456\n",
      "Epoch 220, loss = 5.616990089416504, sim(lo, lu) = 0.9673978090286255\n",
      "Epoch 230, loss = 5.613097667694092, sim(lo, lu) = 0.9679328203201294\n",
      "Epoch 240, loss = 5.60905122756958, sim(lo, lu) = 0.9684563279151917\n",
      "Epoch 250, loss = 5.60484504699707, sim(lo, lu) = 0.9689689874649048\n",
      "Epoch 260, loss = 5.600475788116455, sim(lo, lu) = 0.969470739364624\n",
      "Epoch 270, loss = 5.595938682556152, sim(lo, lu) = 0.9699620604515076\n",
      "Epoch 280, loss = 5.5912299156188965, sim(lo, lu) = 0.9704431295394897\n",
      "Epoch 290, loss = 5.586347579956055, sim(lo, lu) = 0.970914363861084\n",
      "Epoch 300, loss = 5.581287860870361, sim(lo, lu) = 0.9713758230209351\n",
      "Epoch 310, loss = 5.576047897338867, sim(lo, lu) = 0.9718281030654907\n",
      "Epoch 320, loss = 5.570630073547363, sim(lo, lu) = 0.9722710847854614\n",
      "Epoch 330, loss = 5.565031051635742, sim(lo, lu) = 0.9727052450180054\n",
      "Epoch 340, loss = 5.559253215789795, sim(lo, lu) = 0.9731305837631226\n",
      "Epoch 350, loss = 5.553300380706787, sim(lo, lu) = 0.973547637462616\n",
      "Epoch 360, loss = 5.547176361083984, sim(lo, lu) = 0.9739564061164856\n",
      "Epoch 370, loss = 5.540889263153076, sim(lo, lu) = 0.974357008934021\n",
      "Epoch 380, loss = 5.534447193145752, sim(lo, lu) = 0.9747499227523804\n",
      "Epoch 390, loss = 5.527862071990967, sim(lo, lu) = 0.975135326385498\n",
      "Epoch 400, loss = 5.521152019500732, sim(lo, lu) = 0.9755131006240845\n",
      "Epoch 410, loss = 5.514333248138428, sim(lo, lu) = 0.9758837819099426\n",
      "Epoch 420, loss = 5.5074286460876465, sim(lo, lu) = 0.9762474298477173\n",
      "Epoch 430, loss = 5.500463485717773, sim(lo, lu) = 0.9766041040420532\n",
      "Epoch 440, loss = 5.493468284606934, sim(lo, lu) = 0.9769541025161743\n",
      "Epoch 450, loss = 5.486476421356201, sim(lo, lu) = 0.9772975444793701\n",
      "Epoch 460, loss = 5.479523181915283, sim(lo, lu) = 0.9776345491409302\n",
      "Epoch 470, loss = 5.472649097442627, sim(lo, lu) = 0.9779654741287231\n",
      "Epoch 480, loss = 5.4658918380737305, sim(lo, lu) = 0.9782900214195251\n",
      "Epoch 490, loss = 5.459291934967041, sim(lo, lu) = 0.9786088466644287\n"
     ]
    }
   ],
   "source": [
    "embedding_C, context_W = train(center_list, context_list, 500, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# obj0, obj1, obj2 are created here...\n",
    "\n",
    "def save_pickle(idx = 0):\n",
    "    global tmp_c, tmp_w\n",
    "    # Saving the objects:\n",
    "    with open(f'objs-{idx}.pkl', 'wb') as f:\n",
    "        pickle.dump([tmp_c, tmp_w], f)\n",
    "\n",
    "def load_pickle(idx = 0):\n",
    "    global tmp_c, tmp_w\n",
    "    # Getting back the objects:\n",
    "    with open(f'objs-{idx}.pkl', 'rb') as f:\n",
    "        tmp_c, tmp_w = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pickle(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_diff(word_1, word_2):\n",
    "    vector_1 = embedding_C[word_id[word_1]]\n",
    "    vector_2 = embedding_C[word_id[word_2]]\n",
    "    print(fun.cosine_similarity(vector_1, vector_2, dim = 0))\n",
    "\n",
    "\n",
    "\n",
    "def show_vector(word_1):\n",
    "    vector_1 = embedding_C[word_id[word_1]].detach().numpy()\n",
    "    print(vector_1)\n",
    "\n",
    "get_diff(\"kek\", \"kayak\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('3.9.7')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "467d8379f95ad7eb63f1dc50c7908a840ac7d6ef9b8b78dbe6ade2a8663db7de"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
