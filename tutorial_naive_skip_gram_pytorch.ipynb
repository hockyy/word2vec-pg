{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZPmqXlRvqBc"
      },
      "source": [
        "# Naive Implementation of Skip-Gram Language Model\n",
        "\n",
        "Alfan F. Wicaksono (Fasilkom UI)\n",
        "\n",
        "Di tutorial ini, kita akan belajar membangun Word2Vec's Skip-Gram model \"from-the-scratch\". Implementasi yang dilakukan adalah versi Naive dimana output layer masih berupa softmax layer biasa yang mempunyai ukuran sebesar ukuran vocabulary $|V|$. Kenyataannya, arsitektur ini akan sangat lambat jika $|V|$ sangat besar. Implementasi yang jauh lebih efisien adalah dengan Hierarchical Softmax atau dengan menggunakan NCE (Noise-Contrastive Estimation) loss function.\n",
        "\n",
        "Walaupun implementasi pada tutorial ini adalah versi Naive, tutorial ini memberikan gambaran detail bagaimana Word2Vec's Skip-Gram bekerja (you will look inside the black box). Jika Anda memahami tutorial ini, Anda juga bisa memahami bagaimana model-model lain yang masih satu family seperti Continuous Bag-of-Words model dan FastText model. Serta, tutorial ini memberikan fondasi untuk memahami language model yang lebih state-of-the-art seperti ELMO, bahkan BERT dan segala variannya."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NcyNXu-43-hl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as fun\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3ONxk9WAvcPp"
      },
      "outputs": [],
      "source": [
        "corpus = [\"saya tidak makan\",\n",
        "          \"kamu tidak pergi\",\n",
        "          \"saya gk makan\",\n",
        "          \"kamu gk pergi\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wl28FuXLw3YF",
        "outputId": "09951ffc-09d5-4804-bf14-3a719604a891"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'saya': 0, 'tidak': 1, 'makan': 2, 'kamu': 3, 'pergi': 4, 'gk': 5}\n",
            "{0: 'saya', 1: 'tidak', 2: 'makan', 3: 'kamu', 4: 'pergi', 5: 'gk'}\n"
          ]
        }
      ],
      "source": [
        "def create_vocab(corpus):\n",
        "  \"\"\"\n",
        "  membuat vocabulary, dengan 2 info penting: \n",
        "      1. word_id -> mapping kata ke id (int)\n",
        "      2. id_word -> sebaliknya, mapping dari id ke word\n",
        "  \"\"\"\n",
        "  word_id = {}\n",
        "  i = 0\n",
        "  for sentence in corpus:\n",
        "      for word in sentence.split():\n",
        "          if word not in word_id:\n",
        "              word_id[word] = i\n",
        "              i += 1\n",
        "  id_word = {id:word for (word, id) in word_id.items()}\n",
        "  return word_id, id_word\n",
        "\n",
        "word_id, id_word = create_vocab(corpus)\n",
        "print(word_id)\n",
        "print(id_word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUB4AdM2z1u6",
        "outputId": "836c83a6-e8a7-4b3a-9fab-cfcf1f2e87ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "saya tidak\n",
            "tidak saya\n",
            "tidak makan\n",
            "makan tidak\n",
            "kamu tidak\n",
            "tidak kamu\n",
            "tidak pergi\n",
            "pergi tidak\n",
            "saya gk\n",
            "gk saya\n",
            "gk makan\n",
            "makan gk\n",
            "kamu gk\n",
            "gk kamu\n",
            "gk pergi\n",
            "pergi gk\n"
          ]
        }
      ],
      "source": [
        "# dataset generator\n",
        "def iter_dataset(corpus, context_window = 1):\n",
        "  for sentence in corpus:\n",
        "    words = sentence.split()\n",
        "    for i in range(len(words)):\n",
        "      center_word = words[i]\n",
        "      # cari konteks ke kiri\n",
        "      j = i - 1\n",
        "      while (j >= i - context_window) and (j >= 0):\n",
        "        context_word = words[j]\n",
        "        yield (center_word, context_word)\n",
        "        j -= 1\n",
        "      # cari konteks ke kanan\n",
        "      j = i + 1\n",
        "      while (j <= i + context_window) and (j < len(words)):\n",
        "        context_word = words[j]\n",
        "        yield (center_word, context_word)\n",
        "        j += 1\n",
        "\n",
        "for center, context in iter_dataset(corpus):\n",
        "  print(center, context)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75aweUDD58B-",
        "outputId": "21e7450d-d805-442e-998d-52f168790290"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([0, 1, 1, 2, 3, 1, 1, 4, 0, 5, 5, 2, 3, 5, 5, 4])\n",
            "tensor([1, 0, 2, 1, 1, 3, 4, 1, 5, 0, 2, 5, 5, 3, 4, 5])\n"
          ]
        }
      ],
      "source": [
        "# membuat dataset\n",
        "X = []\n",
        "Y = []\n",
        "for center, context in iter_dataset(corpus):\n",
        "  X.append(word_id[center])\n",
        "  Y.append(word_id[context])\n",
        "\n",
        "X_ctr = torch.tensor(X)\n",
        "Y_ctx = torch.tensor(Y)\n",
        "\n",
        "print(X_ctr)\n",
        "print(Y_ctx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0pcnTfL3wJy",
        "outputId": "d37dad07-a802-4de4-fe03-d6c090dcb6f1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1, 0, 0, 0, 0, 0],\n",
              "        [0, 1, 0, 0, 0, 0],\n",
              "        [0, 1, 0, 0, 0, 0],\n",
              "        [0, 0, 1, 0, 0, 0],\n",
              "        [0, 0, 0, 1, 0, 0],\n",
              "        [0, 1, 0, 0, 0, 0],\n",
              "        [0, 1, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 1, 0],\n",
              "        [1, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 1],\n",
              "        [0, 0, 0, 0, 0, 1],\n",
              "        [0, 0, 1, 0, 0, 0],\n",
              "        [0, 0, 0, 1, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 1],\n",
              "        [0, 0, 0, 0, 0, 1],\n",
              "        [0, 0, 0, 0, 1, 0]])"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab_size = len(word_id)\n",
        "\n",
        "def get_input_tensor(tensor):\n",
        "    return fun.one_hot(tensor, num_classes = vocab_size)\n",
        "\n",
        "get_input_tensor(X_ctr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qBPFkqJ78V_",
        "outputId": "926e9b16-8fe0-47a3-e319-8168f62f3f27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "C shape is: torch.Size([6, 5]), W shape is: torch.Size([5, 6])\n",
            "tensor([[ 0.0211,  0.0985, -0.0455, -0.0255,  0.0694],\n",
            "        [-0.0975, -0.0400,  0.0332,  0.0531, -0.0291],\n",
            "        [-0.0093,  0.0155, -0.0791,  0.0351,  0.0700],\n",
            "        [-0.0768, -0.0659, -0.0999,  0.0133,  0.0429],\n",
            "        [-0.0937, -0.0536,  0.0462, -0.0990, -0.0470],\n",
            "        [ 0.0778,  0.0194,  0.0897, -0.0568, -0.0445]], requires_grad=True)\n",
            "tensor([[-0.0442,  0.0224, -0.0367,  0.0495,  0.0488, -0.0330],\n",
            "        [-0.0743, -0.0405, -0.0746,  0.0550, -0.0811,  0.0286],\n",
            "        [-0.0379, -0.0178, -0.0070, -0.0600, -0.0414,  0.0646],\n",
            "        [ 0.0775,  0.0605, -0.0092,  0.0653, -0.0594, -0.0311],\n",
            "        [-0.0429,  0.0704,  0.0887, -0.0201,  0.0433, -0.0915]],\n",
            "       requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "EMBEDDING_DIMS = 5\n",
        "\n",
        "\n",
        "initrange = 0.5 / EMBEDDING_DIMS\n",
        "\n",
        "# trick: di awal, mode gradient jangan diaktifkan dahulu, karena ingin\n",
        "# memodifikasi nilai di C dan W secara in-place\n",
        "C = torch.rand(vocab_size, EMBEDDING_DIMS, requires_grad = False)\n",
        "W = torch.rand(EMBEDDING_DIMS, vocab_size, requires_grad = False)\n",
        "\n",
        "# agar nilai awal parameter di C dan W berkisar di antara -initrange hingga +initrange\n",
        "C = -2 * initrange * C + initrange\n",
        "W = -2 * initrange * W + initrange\n",
        "\n",
        "# setelah operasi in-place modification, baru kita set mode gradient\n",
        "C.requires_grad = True\n",
        "W.requires_grad = True\n",
        "\n",
        "print(f'C shape is: {C.shape}, W shape is: {W.shape}')\n",
        "print(C)\n",
        "print(W)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "vvGVoYYseVZQ"
      },
      "outputs": [],
      "source": [
        "# Softmax\n",
        "def softmax(x):\n",
        "  maxes = torch.max(x, 1, keepdim=True)[0]\n",
        "  x_exp = torch.exp(x - maxes)\n",
        "  x_exp_sum = torch.sum(x_exp, 1, keepdim = True)\n",
        "  return x_exp / x_exp_sum\n",
        "\n",
        "# Categorical Cross Entropy Loss\n",
        "def CCEloss(Y_pred, Y_true):\n",
        "    m = Y_pred.size()[0]\n",
        "    return -(1 / m) * torch.sum(Y_true * torch.log(Y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3TXfa3gydlXJ",
        "outputId": "cf452dba-6e5e-4054-fc9d-32f2d585a1d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, loss = 1.7935898303985596, sim(tidak, gk) = -0.4133906364440918\n",
            "Epoch 10, loss = 1.7917194366455078, sim(tidak, gk) = -0.4798831641674042\n",
            "Epoch 20, loss = 1.7898756265640259, sim(tidak, gk) = -0.4558359980583191\n",
            "Epoch 30, loss = 1.7871145009994507, sim(tidak, gk) = -0.32872533798217773\n",
            "Epoch 40, loss = 1.7820773124694824, sim(tidak, gk) = -0.09139911830425262\n",
            "Epoch 50, loss = 1.7723863124847412, sim(tidak, gk) = 0.21741338074207306\n",
            "Epoch 60, loss = 1.7537850141525269, sim(tidak, gk) = 0.5089050531387329\n",
            "Epoch 70, loss = 1.719202995300293, sim(tidak, gk) = 0.7173287272453308\n",
            "Epoch 80, loss = 1.659062385559082, sim(tidak, gk) = 0.8408175706863403\n",
            "Epoch 90, loss = 1.5662643909454346, sim(tidak, gk) = 0.9070404767990112\n",
            "Epoch 100, loss = 1.4478002786636353, sim(tidak, gk) = 0.9411842823028564\n",
            "Epoch 110, loss = 1.3298448324203491, sim(tidak, gk) = 0.9588562846183777\n",
            "Epoch 120, loss = 1.2377886772155762, sim(tidak, gk) = 0.968408465385437\n",
            "Epoch 130, loss = 1.1765074729919434, sim(tidak, gk) = 0.9739628434181213\n",
            "Epoch 140, loss = 1.137902021408081, sim(tidak, gk) = 0.9774676561355591\n",
            "Epoch 150, loss = 1.1133208274841309, sim(tidak, gk) = 0.9798480272293091\n",
            "Epoch 160, loss = 1.0970847606658936, sim(tidak, gk) = 0.9815657138824463\n",
            "Epoch 170, loss = 1.0859030485153198, sim(tidak, gk) = 0.9828653335571289\n",
            "Epoch 180, loss = 1.0778956413269043, sim(tidak, gk) = 0.9838868975639343\n",
            "Epoch 190, loss = 1.071962594985962, sim(tidak, gk) = 0.9847142696380615\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 200\n",
        "LEARNING_RATE = 0.2\n",
        "LR_DECAY = 0.99\n",
        "\n",
        "for i in range(EPOCHS):\n",
        "  X = get_input_tensor(X_ctr).float()\n",
        "  Y = get_input_tensor(Y_ctx).float()\n",
        "\n",
        "  h = X.mm(C)\n",
        "  Y_pred = softmax(h.mm(W))\n",
        "\n",
        "  loss = CCEloss(Y_pred, Y)\n",
        "  loss.backward()\n",
        "\n",
        "  # update C dan W\n",
        "  with torch.no_grad():\n",
        "    C -= LEARNING_RATE * C.grad\n",
        "    W -= LEARNING_RATE * W.grad\n",
        "    \n",
        "    C.grad.zero_()\n",
        "    W.grad.zero_()\n",
        "\n",
        "  if i % 10 == 0:\n",
        "    # kita coba lihat progress cosine similarity\n",
        "    # antara embedding \"tidak\" dengan \"nggak\"\n",
        "    vector_tidak = C[word_id[\"tidak\"]]\n",
        "    vector_gk = C[word_id[\"gk\"]]\n",
        "    sim = fun.cosine_similarity(vector_tidak, vector_gk, dim = 0)\n",
        "    print(f'Epoch {i}, loss = {loss}, sim(tidak, gk) = {sim}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bGSQ63Uu8WgV",
        "outputId": "16573243-60f9-4d60-bb07-21d09381dee7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 0.5922387  -0.5567056   0.54271483  0.39649314 -0.5429961 ]\n",
            "[ 0.7301304  -0.474415    0.5729187   0.27293417 -0.53294003]\n"
          ]
        }
      ],
      "source": [
        "# vector \"tidak\" dan \"gk\"\n",
        "vector_tidak = C[word_id[\"tidak\"]].detach().numpy()\n",
        "vector_gk = C[word_id[\"gk\"]].detach().numpy()\n",
        "\n",
        "print(vector_tidak)\n",
        "print(vector_gk)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
